# 1. Introduction
- field: Image synthesis
	- 最引人注目
	- 但计算需求大。
	- 高分辨率图像合成
		- 复杂自然景观
		- 主流做法：提升 基于相似度模型 的规模
			- 包含数十亿参数的自回归的 transformers 。
			- GAN 的具有潜力的结果被认为是会受到限制的。
				- 数据的可变性相对有限
				- 对抗学习策略不适用于：
					- 模拟复杂、多模态分布
			- 扩散模型
				- 基于层级的去噪自编码器
				- 在图像合成、超分辨率等场景中很强
				- 没有条件也能实现图像修复和色彩填充。
				- 凭借大程度的参数共享
					- 不会发生模式崩溃。
					- 不会出现像 GAN 一样的训练不稳定。
					- 能够模拟复杂的自然图像分布
					- 无需数十亿的参数
- 大众化的高分辨率图像生成
	- 扩散模型属于基于相似度的模型
		- 在模拟不易察觉的细节时，太费资源
			- weighted variational objective 通过降采样起初的去噪步骤来解决，DM还是会有大体量的计算需求。
			- 在RGB图像的高维空间中， 评估函数和梯度计算函数都需要大量的重复。
			练需要数百个GPU天，V100训练需要150~1000 GPU天
			- 在输入空间的噪声版本上进行重复评估，同样会导致昂贵的代价。
			- 一张A100 GPU 生成50, 000 张样本需要将近5天
	- 为了增加这种powerful模型的可用性， 降低计算资源消耗
	- 同时减少训练和采样的计算复杂度
	- 可用性的关键是　减少 DM　的计算需求，而无需降低其性能。
- 跳出像素空间，并进入潜在空间
	- model 的失真程度与 rate 之间的权衡。
- 基于相似度的模型
	- 学习分为两阶段
	- 阶段1：感知压缩阶段
		- 删除高频细节
		- 但是仍然学习少量语义变化。
	- 阶段2：
		- 具有生成能力的模型 学习 数据的语义和概念性组合。
- 首先找到一个感知上等价，但是计算上更适合 的空间
- 其次在这个空间上训练一个 DM，做高分辨率图像的生成。

- train into 2 distinct phases:
	- 一个自编码器
		- 提供低维表征空间
		- 等价于数据空间。
		- 不需要依赖过度的空间压缩。
	- 在学到的潜在空间中训练 DM
		- 在空间维度上展现更好的缩放特质。
		- 降低的复杂度也能从潜在空间中通过一个单分支网络通路提供高效的图像生成。
	- 成为潜在扩散模型 LDM

- 贡献
	- 与 基于 transformer 的方案相反
		- LDM 在更高维度上更适合。
		- 因此LDM能够在压缩级别上更忠实且详细地重构。
			- 比DALL-E和VQGAN更好。
		- 在百万级像素图像的高分辨率合成上更有效。
		- 