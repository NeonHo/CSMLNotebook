# 1. Introduction
- field: Image synthesis
	- 最引人注目
	- 但计算需求大。
	- 高分辨率图像合成
		- 复杂自然景观
		- 主流做法：提升 基于相似度模型 的规模
			- 包含数十亿参数的自回归的 transformers 。
			- GAN 的具有潜力的结果被认为是会受到限制的。
				- 数据的可变性相对有限
				- 对抗学习策略不适用于：
					- 模拟复杂、多模态分布
			- 扩散模型
				- 基于层级的去噪自编码器
				- 在图像合成、超分辨率等场景中很强
				- 没有条件也能实现图像修复和色彩填充。
				- 凭借大程度的参数共享
					- 不会发生模式崩溃。
					- 不会出现像 GAN 一样的训练不稳定。
					- 能够模拟复杂的自然图像分布
					- 无需数十亿的参数
- 大众化的高分辨率图像生成
	- 扩散模型属于基于相似度的模型
		- 在模拟不易察觉的细节时，太费资源
			- weighted variational objective 通过降采样起初的去噪步骤来解决，DM还是会有大体量的计算需求。
			- 在RGB图像的高维空间中， 评估函数和梯度计算函数都需要大量的重复。
			练需要数百个GPU天，V100训练需要150~1000 GPU天
			- 在输入空间的噪声版本上进行重复评估，同样会导致昂贵的代价。
			- 一张A100 GPU 生成50, 000 张样本需要将近5天
	- 为了增加这种powerful模型的可用性， 降低计算资源消耗
	- 同时减少训练和采样的计算复杂度
	- 可用性的关键是　减少 DM　的计算需求，而无需降低其性能。
- 跳出像素空间，并进入潜在空间
	- model 的失真程度与 rate 之间的权衡。
- 基于相似度的模型
	- 学习分为两阶段
	- 阶段1：感知压缩阶段
		- 删除高频细节
		- 但是仍然学习少量语义变化。
	- 阶段2：
		- 具有生成能力的模型 学习 数据的语义和概念性组合。
- 首先找到一个感知上等价，但是计算上更适合 的空间
- 其次在这个空间上训练一个 DM，做高分辨率图像的生成。

- train into 2 distinct phases:
	- 一个自编码器
		- 提供低维表征空间
		- 等价于数据空间。
		- 不需要依赖过度的空间压缩。
	- 在学到的潜在空间中训练 DM
		- 在空间维度上展现更好的缩放特质。
		- 降低的复杂度也能从潜在空间中通过一个单分支网络通路提供高效的图像生成。
	- 成为潜在扩散模型 LDM

- 贡献
	- 与 基于 transformer 的方案相反
		- LDM 在更高维度上更适合。
		- 因此LDM能够在压缩级别上更忠实且详细地重构。
			- 比DALL-E和VQGAN更好。
		- 在百万级像素图像的高分辨率合成上更有效。
	- 在多任务上获得有竞争力的性能
		- 无条件图像合成
		- 图像修复
		- 随机超分辨率
		- 且只需要很低的计算代价。
		- 相比于基于像素的扩散模型， LDM 也能大幅降低推断代价。
	- 相比于此前既训练编解码器结构，又引入基于分值的先验。
		- LDM 不需要对重建能力和生成能力进行精细的加权处理。
		- 这保证了忠实的重构
		- 并且在潜在空间上仅仅需要很小的正则化。
	- LDM在条件密集的任务，仍然可以使用卷积的风格，渲染出大尺寸的一致的图像。
		- 大尺寸值将近$1024\times 1024$ 
	- 设计一种基于交叉注意力机制的通用调节机制
		- 实现多模态训练
		- 用于训练条件概率、文生图、布局结构生成图 模型
	- 发布代码用于多种任务.
# 2. Related work
- 用于图像合成 的 生成模型
	- 高分辨率挑战大
	- GAN 网络在高分辨率图像上有很高效的采样
		- 但是不好优化
		- 难以提取出整个数据的分布。
	- 基于相似度的方法
		- 强调优秀的密度估计
		- 这使得优化器能够更规范地执行
	- 变分自编码器和基于 flow 的模型
		- 能够做高分辨率的高效生成
		- 但是采样质量与GAN无法匹敌
	- 自回归模型
		- 实现了高性能的密度估计
		- 计算需求和序列采样过程使得他们只能局限在低分辨率上。
		- 由于基于像素表征的特征空间中
			- 几乎感受不到高频细节
			- 最大相似度训练策略可能会使得拟合花费远超于本应该的训练时长
		- 为了提高分辨率，一些二阶段的自回归方法拟合图像的潜在特征空间而不是原本的像素点。
- 扩散概率模型 DM
	- 密度估计和采样质量上都是 SOTA
		- 来源于 backbone is U-Net
		- 这种 backbone 与 类似图像数据的归纳偏好[[Inductive Biases]] 自然契合。
		- 最好的合成质量来自：
			- 用一个重新加权的目标进行训练
			- DM充当一个有损压缩器
				- 牺牲图像质量来换取压缩能力
				- 缺点是
					- 推断的低速
						- 可以被先进的采样策略和分层方法缓解。
					- 训练的高代价
						- 高分辨率图像上需要计算昂贵的梯度。
				- 这些缺点被 LDM 解决了
					- 低维度的潜在空间上进行压缩
					- 这使得
						- 训练上更便宜，
						- 推断上在保证合成质量的前提上还能做加速。
- 两阶段图像生成
	- 避免单一生成方法的弊端，大量研究开始将不同的方法一起应用
		- 两阶段 更高效且性能更强
	- VQ-VAE 使用自回归模型学习 
		- 在离散潜在空间中学习一个表现力强的先验
		- 有工作：
			- 扩展这个方法到 文生图 的生成工作
				- 通过学习一个 离散化的图像空间 和 文本表征 两种数据的联合分布
			- 更通用的工作
				- 使用条件可逆网络提供
					- 从潜在空间到多种领域的通用转换。
	- VQGAN
		- 第一阶段
			- 使用一个第一带有对抗和感知目标的阶段
				- 这能将自回归 transformers 提升至大图。
				- 然而，对于可行的ARM训练所需的高压缩率，引入了数十亿个可训练参数，限制了这些方法的整体性能，而较低的压缩率则以高计算成本为代价。
		- LDM 不需要这种平衡取舍
			- 由于卷积 backbone，更温和地扩展到更高维度的潜在空间。
			- 在保证忠实重构的时候
				- 无需遗留下太多的感知压缩工作给到生成扩散模型，
				- 可以自由选择压缩级别来让第一阶段得到学习。
			- 虽然也存在一些联合学习编解码器和基于分值先验的工作存在，并且比 LDM性能好，但是仍然需要困难地在重构和生成能力上进行权重调整。
# 3. Method
- 发现：
	- 尽管 DM 通过欠采样对应的损失项，从而忽略 感知不相关的 细节
	- DM 仍然需要在像素空间当中使用具有代价的函数评估。
		- 需要巨大的计算需求和能量资源
- LDM 绕过这个缺陷
	- 从生成学习阶段显式地分离出压缩工作。
	- 实现方法
		- 在一个与图像空间感知等价的空间上学习一个自编码模型。
		- 能够提供一个大幅的计算复杂度衰减。
- 好处多多：
	- 因为在低维空间上采样，获得了一个计算效率更高的 DM
	- 利用了来自 UNet 的归纳偏置
		- 在带有空间结构的数据上尤其高效
		- 完全不需要 那些其他方法必备的 降低质量的 粗暴的 压缩等级
	- 通用目标的压缩模型，
		- 它的潜在空间将被用于训练多个生成模型
			- 下游任务都能用
			- 例如 但图像 CLIP指导 图像合成。
## 3.1. 感知图像压缩
- 基于此前的工作
- 一个自编码器
	- 联合训练：
		- 感知损失
		- 基于patch