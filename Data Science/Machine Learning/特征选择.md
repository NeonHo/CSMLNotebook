### 特征选择的三种方法：过滤法（Filter）、包装法（Wrapper）、嵌入法（Embedded）

#### 1. 过滤法（Filter）
过滤法是一种在模型训练之前对特征进行评估和筛选的方法，独立于具体的机器学习模型。它通过统计测试来评估每个特征的重要性，常见的方法包括：
- **方差选择法**：通过计算特征的方差，去除方差小于某个阈值的特征。
- **相关系数法**：计算特征与目标变量之间的皮尔逊相关系数，选择相关性高的特征。
- **互信息法**：衡量特征与目标变量之间的信息共享程度，适用于非线性关系。[[互信息]]
- **卡方检验**：用于分类问题，评估特征与目标变量之间的独立性。

**适用场景**：适用于特征数量较多，需要快速筛选出重要特征的场景。

##### Relief
Relief是一种著名的过滤式特征选择方法。它由Kira和Rendell在1992年提出，最初是为具有离散或数值特征的二分类问题设计的。 Relief算法通过计算特征得分来评估特征的重要性，进而选择得分高的特征用于后续建模。其具体做法是从训练集中随机选择一个样本R，然后从与R同类的样本中寻找最近邻样本H（称为Near Hit），从与R不同类的样本中寻找最近邻样本M（称为Near Miss）。接着根据以下规则更新每个特征的权重：如果R和Near Hit在某个特征上的距离小于R和Near Miss在该特征上的距离，说明该特征对区分同类和不同类的最近邻有益，会增加该特征的权重；反之则降低该特征的权重。重复上述过程多次，最后得到各特征的平均权重，权重越大表示该特征的分类能力越强。 Relief与二分类的关系密切，它最初就是针对二分类问题设计的。该算法可以帮助二分类模型选择更具区分度的特征，去除对分类贡献较小的特征，从而提高二分类模型的性能和效率，减少计算量和过拟合的风险等。例如，在文本二分类任务中，通过Relief算法可以筛选出与类别相关性高的词语特征，摒弃那些无关紧要的词语，让分类器能更专注于关键特征，提升分类准确率。 后来基于Relief算法，又发展出了Relief-F等拓展算法，Relief-F算法可以处理多分类问题，但Relief算法本身主要还是应用于二分类场景。

#### 2. 包装法（Wrapper）
包装法将特征选择问题视为一个搜索问题，通过多次训练模型来评估特征子集的性能。常见的方法包括：
- **递归特征消除（RFE）**：从完整的特征集开始，递归地移除最不重要的特征，直到达到所需的特征数量。
- **递归特征添加（SFS）**：从空特征集开始，逐步添加最有价值的特征。
- **基于遗传算法的特征选择**：使用遗传算法搜索最优特征子集。

**优点**：能够找到最优的特征子集，考虑特征之间的相互关系。
**缺点**：计算成本高，容易过拟合。
**适用场景**：适用于特征数量较少，计算资源充足的情况。

#### 3. 嵌入法（Embedded）
嵌入法在模型训练过程中同时进行特征选择，通过模型的权重系数来评估特征的重要性。常见的方法包括：
- **Lasso回归**：通过L1正则化将某些特征的权重压缩为零。
- **Ridge回归**：通过L2正则化控制特征权重。
- **ElasticNet**：结合L1和L2正则化。
- **决策树和随机森林**：根据特征在树中的重要性进行选择。

**优点**：结果更精确，能够处理特征之间的相关性。
**缺点**：计算成本较高，权重系数的阈值难以确定。
**适用场景**：适用于数据量较大，需要精确特征选择的场景。

### 总结
- **过滤法**：快速筛选特征，适用于特征数量较多的场景。
- **包装法**：寻找最优特征子集，但计算成本高，适用于特征数量较少的场景。
- **嵌入法**：结合模型训练进行特征选择，结果更精确，适用于数据量较大的场景。



特征选择在子集生成与搜索方面引入了很多人工智能搜索技术, 如：
- 分支界限法,
- 浮动搜索法
- 等;
在子集评价方法则采用了很多源于信息论的准则, 如：
- 信息熵、
- AIC [[AIC & BIC]]
- 等。

特征选择的主要目的是:
- 减少特征的数量,
- 降低特征维度,
- 使模型泛化能力更强,
- 减少过拟合。

子集搜索中,逐渐增加相关特征的策略称为前向搜索。


正向选择(Forward Selection)是首先选择一个特征,每个特征都试一遍,选择对模型准确率提升最高的那个特征;然后再在这个特征基础上添加另外一个特征,方法类似,直到模型准确率不再提示为止。

反向消除(Backward Elimination)是首先包含了所有的特征,然后尝试删除每个特征,最终删掉对模型准确率提升最高的一个特征(因为删除这个特征,模型准确率反而增加了,说明是无用特征)。如此类推,直到删除特征并不能提升模型为止。相对于 Forward Selection,Backward Elimination 的优点在于其允许一些低贡献值的特征能够进到模型中去(有时候低贡献值的特征能在组合中有更大的贡献值,而 Forward Selection 忽略了这种组合的可能性),因此Backward Elimination能够避免受一两个占主导地位的特征的干扰。

另外还有一种特征选择方法是 Stepwise,该方法结合上述两者的方法,新加入一个特征之后,再尝试删去一个特征,直至达到某个预设的标准。这种方法的缺点是,预设的标准不好定,而且容易陷入到过拟合当中。除此之外,也可以使用基于相关性的特征选择,可以去除多重线性特征。