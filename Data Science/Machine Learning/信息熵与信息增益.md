一句话先给出结论：  
**信息熵**衡量“一团数据有多混乱”，**信息增益**衡量“用这个特征做一次分裂后，混乱减少了多少”。

下面把概念拆成 4 步，配合一个极简例子，帮助你直观理解。

---

### 1. 信息熵：混乱程度温度计  
- 取值范围：$[0, \log_2 k]$（$k$ 为类别数）。  
- 0 表示“纯得不能再纯”（所有样本同类别）。  
- 越大表示“越鱼龙混杂”。

公式：  
$$
\text{Entropy}(D)=-\sum_{i=1}^{k} p_i \log_2 p_i
$$
其中 $p_i$ 是第 $i$ 类样本在数据集 $D$ 中的占比。

---

### 2. 信息增益：一次分裂的“净化量”  
- 先把父节点的熵记下来；  
- 再把分裂后每个子节点的熵按样本数加权平均；  
- 两者相减就是信息增益。  

$$
\text{Gain}(D, A)=\text{Entropy}(D)-\sum_{v\in \text{Values}(A)}\frac{|D_v|}{|D|}\text{Entropy}(D_v)
$$
Gain 越大 → 用这个特征分裂后纯度提升越显著 → 就越值得在这里切一刀。

---

### 3. 举个 1 分钟能算完的例子  
数据集：10 条样本，二分类（Yes/No）。  
- 原始分布：7 Yes，3 No  
- 熵：$-(0.7\log_2 0.7 + 0.3\log_2 0.3)\approx 0.881$

现在考虑特征 A（只有两个取值 T/F）：  
- T 分支：6 条样本 → 6 Yes，0 No → 熵 = 0  
- F 分支：4 条样本 → 1 Yes，3 No → 熵 ≈ 0.811  

信息增益：  
$$
0.881 - \left(\frac{6}{10}\cdot 0 + \frac{4}{10}\cdot 0.811\right)\approx 0.881 - 0.324 = 0.557
$$
0.557 就是这次分裂带来的“混乱减少量”。

---

### 4. 小结口诀  
- 熵高 → 乱；熵低 → 纯。  
- 信息增益 → 用某特征分裂后“熵下降了多少”。  
- 决策树在每个节点挑**信息增益最大**（或熵下降最多）的特征做分裂，层层净化，直到叶子足够纯为止。