
Fisher判别分析（Fisher Discriminant Analysis，FDA）与线性判别分析（Linear Discriminant Analysis，LDA）[[LDA 线性判别分析]]在统计学和机器学习领域常被提及，它们的关联与区别容易混淆。下面从**核心思想**、**数学原理**、**应用场景**三个维度进行解析：

## 一、Fisher判别分析（FDA）：寻找最优投影方向

1. **核心思想**  
   FDA由英国统计学家R. A. Fisher于1936年提出，旨在找到一个投影方向，使得**不同类别数据在投影后的均值差异最大**（类间分离度最大化），同时**同一类别数据在投影后的方差最小**（类内紧凑度最大化）。这一方向被称为**Fisher判别准则**。

2. **数学表达**  
   对于二分类问题，假设有两类数据 $C_1$ 和 $C_2$，FDA寻找投影向量 $w$，使得：

   $$
   J(w) = \frac{w^T S_B w}{w^T S_W w}
   $$

   其中：
   - $S_B = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T$ 是**类间散布矩阵**，$\mu_1$ 和 $\mu_2$ 是两类数据的均值向量；
   - $S_W = \sum_{i \in C_1} (x_i - \mu_1)(x_i - \mu_1)^T + \sum_{i \in C_2} (x_i - \mu_2)(x_i - \mu_2)^T$ 是**类内散布矩阵**。

   最优解 $w^*$ 是 $S_W^{-1} S_B$ 的最大特征值对应的特征向量。

## 二、线性判别分析（LDA）：从贝叶斯角度到降维工具

1. **LDA的两种角色**  
   - **分类器（Generative Model）**：基于贝叶斯决策理论，假设不同类别数据服从高斯分布且协方差矩阵相同，通过最大化后验概率进行分类。  
   - **降维方法（Dimensionality Reduction）**：与FDA类似，寻找投影方向使类间方差最大化、类内方差最小化，但数学推导基于概率模型。

2. **与FDA的数学关联**  
   当LDA作为降维方法时，其目标函数与FDA完全一致。对于二分类问题，最优投影方向 $w$ 同样满足：

   $$
   w \propto S_W^{-1} (\mu_1 - \mu_2)
   $$

   因此，**FDA和作为降维方法的LDA在数学上等价**。

## 三、FDA与LDA的关联与区别

| **维度**       | **Fisher判别分析（FDA）**               | **线性判别分析（LDA）**                  |
|----------------|----------------------------------------|------------------------------------------|
| **起源**       | 1936年，纯线性代数视角（投影最大化）  | 1948年，贝叶斯决策理论（概率建模）       |
| **核心目标**   | 最大化类间/类内方差比                   | 作为分类器：最大化后验概率<br>作为降维：与FDA等价 |
| **假设条件**   | 无分布假设，仅需计算均值和协方差       | 假设数据服从高斯分布且协方差相同         |
| **应用场景**   | 纯降维任务（如特征提取）               | 分类任务（如垃圾邮件识别）或降维         |
| **数学形式**   | $J(w) = \frac{w^T S_B w}{w^T S_W w}$   | 作为降维：同FDA<br>作为分类器：$P(y \mid x) \propto P(x \mid y)P(y)$ |


## 五、应用场景对比

1. **FDA的优势**  
   - 无需假设数据分布，适用于非高斯数据。  
   - 纯降维工具，不依赖概率模型，计算简单。

2. **LDA的优势**  
   - 作为分类器时，可输出概率（如 $P(y=1 \mid x)$），便于阈值调整。  
   - 理论上在满足高斯假设时，分类性能最优（贝叶斯最优分类器）。
[[半朴素贝叶斯分类]]
## 六、常见误区澄清

1. **FDA与LDA是否完全等价？**  
   - **作为降维方法**：二者数学上等价，仅推导视角不同（FDA基于线性代数，LDA基于概率）。  
   - **作为分类器**：LDA是完整的概率模型，FDA仅提供投影方向，需额外设计分类规则。

2. **LDA与PCA的区别？**  
   - **LDA**：有监督降维，利用类别信息寻找区分性特征。  
   - **PCA**：无监督降维，仅关注数据方差，不考虑类别。

## 七、总结：如何选择？

- **若目标是降维**：FDA/LDA均可，优先选择实现简单的工具（如Python中的`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`）。  
- **若目标是分类**：  
  - 数据近似高斯分布且协方差相同时，优先用LDA。  
  - 数据复杂或非线性时，考虑非线性方法（如核LDA、SVM）。

FDA与LDA的联系体现了统计学中“线性代数视角”与“概率视角”的统一，二者共同奠定了线性分类与降维的理论基础。
