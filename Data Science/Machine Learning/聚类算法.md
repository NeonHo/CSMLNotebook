
聚类算法是**无监督学习**（Unsupervised Learning）的核心技术之一，其目标是将数据集中的样本按照“内在相似性”自动划分为若干个互不重叠的子集（称为“簇”，Cluster），使得**同一簇内的样本相似度高，不同簇的样本差异大**。它无需预先知道样本的标签，完全依赖数据自身的分布特征进行分组。

### 一、聚类的核心要素
聚类的核心是定义“相似度”（或“距离”），以及如何基于相似度划分簇。

#### 1. 相似度/距离度量
常用的度量方式包括：
[[距离]]
- **欧氏距离（Euclidean Distance）**：最常用，衡量高维空间中两点的直线距离，公式为  
  $$d(x_i, x_j) = \sqrt{\sum_{k=1}^n (x_{ik} - x_{jk})^2}$$  
  适用于连续型特征，对异常值敏感。
- **曼哈顿距离（Manhattan Distance）**：衡量两点在坐标轴上的绝对距离之和，公式为  
  $$d(x_i, x_j) = \sum_{k=1}^n |x_{ik} - x_{jk}|$$  
  适用于网格状数据（如城市道路导航）。
- **余弦相似度（Cosine Similarity）**：衡量两向量的方向一致性，值越接近 1 越相似，公式为  
  $$\cos\theta = \frac{x_i \cdot x_j}{\|x_i\| \|x_j\|}$$  
  适用于高维稀疏数据（如文本向量、图像特征）。
- **杰卡德相似系数（Jaccard Index）**：衡量集合的重叠程度，公式为  
  $$J(A,B) = \frac{|A \cap B|}{|A \cup B|}$$  
  适用于布尔型特征（如用户是否点击某商品）。

#### 2. 簇的定义
不同算法对“簇”的理解不同：
- 划分式算法（如 K-Means）：簇是“围绕质心的密集区域”。
- 密度式算法（如 DBSCAN）：簇是“密度相连的点集”。
- 层次式算法（如 AGNES）：簇是“层次化的嵌套结构”。
- 模型式算法（如 GMM）：簇是“概率分布的样本集合”。

---

### 二、聚类算法的分类与典型算法
根据核心思想，聚类算法可分为 5 大类，以下是最常用的典型算法详解：

#### 1. 基于划分的聚类（Partition-based）
**核心思想**：通过迭代优化，将数据划分为 K 个预定义的簇，使簇内样本的“误差”最小。  
**代表算法：K-Means**

##### K-Means 算法
- **目标**：将数据分为 K 个簇，最小化“簇内平方和（SSE）”：  
  $$\text{SSE} = \sum_{k=1}^K \sum_{x \in C_k} \|x - \mu_k\|^2$$  
  其中 $C_k$ 是第 k 个簇，$\mu_k$ 是簇的质心（均值）。
- **步骤**  
  1. **初始化**：随机选择 K 个样本作为初始质心 $(\mu_1, \mu_2, \dots, \mu_K)$。  
  2. **分配样本**：计算每个样本到 K 个质心的距离，将样本分配到最近的质心所属的簇。  
  3. **更新质心**：重新计算每个簇的质心（该簇所有样本的均值）。  
  4. **迭代收敛**：重复步骤 2 和 3，直到质心位置变化小于阈值，或达到最大迭代次数。
- **优点**  
  - 计算高效（时间复杂度 $O(nKT)$，n 为样本数，T 为迭代次数），适合大数据。  
  - 实现简单，应用广泛。
- **缺点**  
  - 需手动指定 K 值（簇数），且对初始质心敏感（可能收敛到局部最优）。  
  - 只能发现凸形簇（如球形），对非凸、不规则形状的簇效果差。  
  - 对异常值和噪声敏感（质心易被极端值拉偏）。

##### K-Means 的改进算法
- **K-Means++**：优化初始质心选择，使质心尽可能远离，减少局部最优问题。  
- **Mini-Batch K-Means**：使用随机小批量样本更新质心，适合百万级以上数据，牺牲少量精度换取速度。  
- **K-Medoids（PAM）**：用簇中最具代表性的样本（medoid）代替均值作为质心，抗噪声能力更强，但计算更慢。

---

#### 2. 基于层次的聚类（Hierarchical-based）
**核心思想**：通过构建层次化的簇结构（树状图），逐步合并或分裂簇，形成“父子关系”的簇。  
**代表算法：AGNES（凝聚式）和 DIANA（分裂式）**[[DIANA]]

##### AGNES（凝聚式层次聚类）
- **思路**：从“每个样本为一个簇”开始，逐步合并最相似的两个簇，直到所有样本合并为一个簇。
- **步骤**  
  1. 初始化：每个样本为一个独立的簇（共 n 个簇）。  
  2. 计算所有簇之间的相似度（如两簇质心的距离）。  
  3. 合并最相似的两个簇，形成新的簇。  
  4. 重复步骤 2 和 3，直到所有样本合并为一个簇（或达到预设簇数）。
- **簇间相似度度量**  
  - 单链接（Single Linkage）：两簇中最近样本的距离（易受噪声影响，可能形成“链式簇”）。  
  - 全链接（Complete Linkage）：两簇中最远样本的距离（簇更紧凑，但对异常值敏感）。  
  - 平均链接（Average Linkage）：两簇所有样本对的平均距离（平衡单链接和全链接的缺点）。
- **优点**  
  - 无需指定 K 值，可生成层次化的簇结构（树状图），便于理解数据层级关系。  
  - 能处理任意形状的簇（取决于相似度度量）。
- **缺点**  
  - 计算复杂度高（$O(n^3)$），不适合样本数超过 1 万的数据集。  
  - 一旦合并/分裂簇，无法回溯，可能积累错误。

---

#### 3. 基于密度的聚类（Density-based）
**核心思想**：簇是“密度高于周围区域的连续区域”，能发现任意形状的簇，并识别噪声。  
**代表算法：DBSCAN**

##### DBSCAN 算法
- **核心概念**  
  - **$\epsilon$（半径）**：定义“邻域”的范围（某样本周围距离小于 $\epsilon$ 的所有点）。  
  - **MinPts（最小点数）**：邻域内至少包含的样本数，才能使中心样本成为“核心点”。  
  - **核心点**：邻域内样本数 ≥ MinPts 的点。  
  - **边界点**：邻域内样本数 < MinPts，但落在某个核心点的邻域内的点。  
  - **噪声点**：既不是核心点，也不是边界点的点。  
  - **密度可达**：若存在核心点链 $p_1, p_2, \dots, p_k$，使 $p_1 = p$，$p_k = q$，且每个 $p_{i+1}$ 在 $p_i$ 的邻域内，则 p 到 q 密度可达。  
  - **密度相连**：若存在点 o，使 p 和 q 都从 o 密度可达，则 p 和 q 密度相连。
- **步骤**  
  1. 随机选择一个未标记的样本点 p。  
  2. 若 p 是核心点，以 p 为起点，将所有密度可达的点合并为一个簇。  
  3. 若 p 是边界点或噪声点，标记为噪声，继续选择下一个未标记点。  
  4. 重复步骤 1-3，直到所有样本点被标记（属于某个簇或噪声）。
- **优点**  
  - 无需指定簇数 K，能自动发现任意形状的簇（如环形、螺旋形）。  
  - 可识别噪声点（异常值），抗噪声能力强。
- **缺点**  
  - 对参数 $\epsilon$ 和 MinPts 敏感（需根据数据分布调整，高维数据中距离度量失效）。  
  - 对密度不均匀的数据效果差（同一 $\epsilon$ 无法适应不同密度的簇）。  
  - 计算复杂度 $O(n^2)$（需计算所有点对的距离），不适合大数据。

---

#### 4. 基于模型的聚类（Model-based）
**核心思想**：假设数据由某个概率模型生成（如多个分布的混合），通过拟合模型参数，将样本分配到最可能的分布中。  
**代表算法：高斯混合模型（GMM）**

##### 高斯混合模型（GMM）
- **假设**：数据由 K 个高斯分布（正态分布）混合生成，每个高斯分布对应一个簇。  
  每个样本 x 的生成概率为：  
  $$p(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)$$  
  其中 $\pi_k$ 是第 k 个高斯分布的权重（$\sum \pi_k = 1$），$\mu_k$ 是均值向量，$\Sigma_k$ 是协方差矩阵。
- **目标**：通过“期望最大化（EM）算法”估计参数 $(\pi_k, \mu_k, \Sigma_k)$，使样本的似然概率最大。
- **步骤（EM 算法）**  
  1. **初始化**：随机设置 K 个高斯分布的参数 $(\pi_k, \mu_k, \Sigma_k)$。  
  2. **E 步（期望）**：计算每个样本属于第 k 个簇的后验概率（“软分配”）：  
     $$\gamma_{ik} = p(z_i = k | x_i) = \frac{\pi_k \mathcal{N}(x_i | \mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(x_i | \mu_j, \Sigma_j)}$$  
     其中 $z_i$ 是样本 i 的潜在簇标签。  
  3. **M 步（最大化）**：更新参数，使似然函数最大：  
     $$\mu_k = \frac{\sum_{i=1}^n \gamma_{ik} x_i}{\sum_{i=1}^n \gamma_{ik}}, \quad \Sigma_k = \frac{\sum_{i=1}^n \gamma_{ik} (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^n \gamma_{ik}}, \quad \pi_k = \frac{1}{n} \sum_{i=1}^n \gamma_{ik}$$  
  4. **迭代收敛**：重复 E 步和 M 步，直到参数变化小于阈值。
- **优点**  
  - 支持“软聚类”（每个样本有属于不同簇的概率），更灵活。  
  - 能捕捉簇的协方差结构（如椭圆形簇）。
- **缺点**  
  - 需指定 K 值，且假设数据服从高斯分布，对非高斯分布的簇（如环形）效果差。  
  - 易陷入局部最优，对初始参数敏感。

---

#### 5. 其他重要聚类算法
- **谱聚类（Spectral Clustering）**：  
  基于图论，将样本视为图的节点，相似度为边的权重，通过特征值分解将高维数据映射到低维，再用 K-Means 聚类。适合高维数据和复杂形状簇，但计算复杂度高。  
- **均值漂移（Mean Shift）**：  
  基于密度梯度上升，通过迭代寻找数据的“密度峰值”（簇中心），无需指定 K 值，能发现任意形状簇，但对带宽参数敏感，计算较慢。

---

### 三、聚类效果的评估指标
由于聚类是无监督学习（无真实标签），评估需结合“内部指标”（数据自身特征）和“外部指标”（若有标签）。

#### 1. 内部指标（无真实标签时）
- **轮廓系数（Silhouette Coefficient）**：  
  衡量样本与同簇的相似度（a）和与最近异簇的相似度（b），值范围 $[-1,1]$：  
  $$s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}$$  
  越接近 1，聚类效果越好；接近 0 说明样本在簇边界；负数说明可能分配错误。
- **Calinski-Harabasz 指数（CH 指数）**：  
  比值越大越好，反映簇间离散度与簇内离散度的比例：  
  $$\text{CH} = \frac{\text{簇间离差和} / (K-1)}{\text{簇内离差和} / (n-K)}$$
- **Davies-Bouldin 指数（DB 指数）**：  
  衡量簇的平均相似度，值越小越好，反映簇间距离与簇内直径的比例。

#### 2. 外部指标（有真实标签时）
- **调整兰德指数（ARI）**：  
  衡量聚类结果与真实标签的一致性，考虑随机分配的影响，值范围 $[-1,1]$，越接近 1 越好。
- **互信息（MI/NMI）**：  
  衡量两个标签分布的相关性，归一化后（NMI）范围 $[0,1]$，1 表示完全一致。

---

### 四、聚类算法的应用场景
- **客户分群**：根据消费习惯、demographics 等将用户分为不同群体，精准营销。  
- **图像分割**：将图像像素按颜色、纹理聚类，提取目标区域。  
- **文本聚类**：将文档按主题聚类（如新闻分类）。  
- **异常检测**：通过聚类识别远离所有簇的样本（如信用卡欺诈检测）。  
- **基因分析**：将基因表达数据聚类，发现功能相似的基因。

---

### 五、聚类算法的选择建议
| 场景                          | 推荐算法                | 理由                                      |
|-------------------------------|-------------------------|-------------------------------------------|
| 大数据、凸形簇、追求效率      | K-Means / Mini-Batch K-Means | 速度快，适合百万级样本                    |
| 非凸形簇、需识别噪声          | DBSCAN                  | 能发现任意形状簇，抗噪声                  |
| 需层次化结构、小数据          | AGNES                   | 生成树状图，适合探索数据层级关系          |
| 软聚类、高斯分布数据          | GMM                     | 支持概率分配，捕捉簇的协方差              |
| 高维数据、复杂形状簇          | 谱聚类                  | 低维映射后聚类，效果优于传统算法          |

---

### 总结
聚类算法是无监督学习的核心工具，通过挖掘数据的内在结构实现自动分组。不同算法各有侧重：K-Means 高效但适合凸簇，DBSCAN 擅长复杂形状和噪声，GMM 支持软聚类，层次聚类提供层级结构。实际应用中需根据数据规模、簇的形状、是否有标签等选择算法，并通过交叉验证优化参数（如 K、$\epsilon$）。
