RNN（循环神经网络，Recurrent Neural Network）是一类专门用于处理**时序数据**（如文本、语音、时间序列信号等）的神经网络。其核心设计是通过“循环连接”引入“记忆机制”，让模型能利用历史信息处理当前输入，从而天然适配时序数据“前后依赖”的特性。下面从定义、结构、原理、训练、问题及应用等方面详细解析。

---

### **一、什么是 RNN？—— 核心定义与特点**

RNN 的本质是“带记忆的神经网络”。与前馈神经网络（如 [[MLP]]）的“输入→输出”单向传递不同，RNN 在隐藏层中加入**循环连接**（Recurrent Connections），使当前时间步的输出不仅依赖于当前输入，还依赖于**上一个时间步的隐藏状态**（即“记住”过去的信息）。

这种设计使其能解决时序数据的核心挑战：**当前数据的含义与历史数据密切相关**（例如，句子中“他”的指代依赖前文提到的人，股票今日价格受前几日趋势影响）。

---

### **二、RNN 的基础结构与工作原理**

RNN 的结构可从“紧凑形式”和“展开形式”理解，展开形式更易看清时间维度上的依赖关系。

#### 1. 基础结构：从“紧凑”到“展开”

- **紧凑形式**：隐藏层有一个“自循环”连接（即隐藏层输出会反馈到自身），表示“当前隐藏状态依赖历史状态”。
- **展开形式**：按时间步（Time Step）将循环结构“拉开”，每个时间步对应一个输入，隐藏状态在时间步间传递。

下图是典型的 RNN 展开示意图（以处理序列长度为 3 的输入为例）：

```
时间步1：x₁ → [隐藏层] → h₁ → [输出层] → y₁
                          ↑
时间步2：x₂ → [隐藏层] → h₂ → [输出层] → y₂
                          ↑
时间步3：x₃ → [隐藏层] → h₃ → [输出层] → y₃
```

其中：

- $x_t$：第 $t$ 个时间步的输入（如句子中的第 $t$ 个词、时间序列中第 $t$ 时刻的观测值）；
- $h_t$：第 $t$ 个时间步的隐藏状态（“记忆”载体，存储截至 $t$ 时刻的历史信息）；
- $y_t$：第 $t$ 个时间步的输出（如对 $x_t$ 的预测、分类结果）。

#### 2. 核心公式：隐藏状态与输出的计算

RNN 的核心是隐藏状态 $h_t$ 的更新，它同时依赖当前输入 $x_t$ 和上一时刻的隐藏状态 $h_{t-1}$，公式如下：

##### （1）隐藏状态计算

$$
h_t = \sigma(W_{xh} \cdot x_t + W_{hh} \cdot h_{t-1} + b_h)
$$

- $W_{xh}$：输入层到隐藏层的权重矩阵（将输入 $x_t$ 映射到隐藏层空间）；
- $W_{hh}$：隐藏层到自身的循环权重矩阵（将上一时刻隐藏状态 $h_{t-1}$ 传递到当前时刻）；
- $b_h$：隐藏层的偏置项；
- $\sigma$：激活函数（通常用 tanh 或 sigmoid，用于引入非线性，tanh 更常见，因为输出范围对称，便于梯度传递）。

##### （2）输出计算

输出 $y_t$ 由当前隐藏状态 $h_t$ 映射得到：

$$
y_t = \sigma(W_{hy} \cdot h_t + b_y)
$$

- $W_{hy}$：隐藏层到输出层的权重矩阵；
- $b_y$：输出层的偏置项；
- 若为分类任务，输出层可能用 softmax 激活（如文本预测下一个词）；若为回归任务，可能用线性激活（如时序预测）。

##### （3）关键特性：参数共享

与 MLP 不同，RNN 在**所有时间步共享同一套参数**（$W_{xh}, W_{hh}, W_{hy}, b_h, b_y$）。这意味着：

- 无论序列长度是 10 还是 1000，参数总量固定，避免参数爆炸；
- 模型能学习“跨时间步的通用规律”（如语言中“主谓宾”结构在不同句子中重复出现），泛化能力更强。

#### 3. 前向传播过程

以序列 $x_1, x_2, x_3$ 为例，前向传播步骤如下：

1. 初始化：若为序列第一个时间步（$t=1$），无历史隐藏状态，通常设 $h_0 = 0$（零向量）；
2. $t=1$：计算 $h_1 = \sigma(W_{xh}x_1 + W_{hh}h_0 + b_h)$，再计算 $y_1 = \sigma(W_{hy}h_1 + b_y)$；
3. $t=2$：用 $h_1$ 计算 $h_2 = \sigma(W_{xh}x_2 + W_{hh}h_1 + b_h)$，再得 $y_2$；
4. $t=3$：同理，用 $h_2$ 计算 $h_3$ 和 $y_3$；
5. 最终得到整个序列的输出 $y_1, y_2, y_3$。

---

### **三、RNN 的主要类型（按输入输出结构划分）**

根据输入序列（$X$）和输出序列（$Y$）的长度关系，RNN 可分为以下 4 类，适配不同任务：

#### 1. 一对一（One-to-One）

- 结构：单输入 → 单输出（无循环，本质是 MLP 的特例）；
- 示例：图像分类（输入一张图片，输出一个类别）。

#### 2. 一对多（One-to-Many）

- 结构：单输入 → 序列输出；
- 原理：用一个输入初始化隐藏状态，然后逐步生成输出序列；
- 示例：
  - 图像描述（输入一张图片，输出描述句子“一只猫坐在沙发上”）；
  - 音乐生成（输入一个主题，输出一段旋律序列）。

#### 3. 多对一（Many-to-One）

- 结构：序列输入 → 单输出；
- 原理：处理完整个输入序列后，用最终隐藏状态输出结果；
- 示例：
  - 情感分析（输入句子“这部电影太精彩了”，输出“正面”）；
  - 股票趋势预测（输入连续 10 天价格，输出第 11 天涨/跌）。

#### 4. 多对多（Many-to-Many）

- 结构：序列输入 → 序列输出（输入输出长度可相同或不同）；
- 细分场景：
  - 输入输出长度相同：如视频帧标注（每帧输入对应一个标签输出）；
  - 输入输出长度不同：如机器翻译（输入英文句子“Hello world”，输出中文句子“你好世界”），此时需用“编码器-解码器”结构（Encoder-Decoder RNN）：
    - 编码器（Encoder）：处理输入序列，输出一个“上下文向量”（压缩整个输入的信息）；
    - 解码器（Decoder）：用上下文向量初始化，生成输出序列。

#### 5. 双向 RNN（Bidirectional RNN, BRNN）

基础 RNN 仅利用“过去的历史信息”（前向传播），但部分任务需要“未来信息”（如句子填空“我吃了____，它很甜”，需知道后文“它很甜”才能推断空格是“苹果”）。

双向 RNN 通过两个方向的 RNN 解决：

- 前向 RNN（Forward RNN）：从左到右处理序列，捕捉过去信息；
- 后向 RNN（Backward RNN）：从右到左处理序列，捕捉未来信息；
- 每个时间步的输出 $y_t$ 由前向隐藏状态 $\overrightarrow{h_t}$ 和后向隐藏状态 $\overleftarrow{h_t}$ 拼接后计算：

$$
y_t = \sigma(W_{hy} \cdot [\overrightarrow{h_t}; \overleftarrow{h_t}] + b_y)
$$

- 示例：语音识别（需结合前后发音推断当前音节）、命名实体识别（如判断“北京”是地名，需结合前后文）。

---

### **四、RNN 的训练：随时间反向传播（BPTT）**

RNN 的训练目标是最小化预测输出 $y_t$ 与真实标签 $\hat{y}_t$ 的损失（如交叉熵损失、MSE）。由于存在循环结构，训练需用**随时间反向传播（Backpropagation Through Time, BPTT）** 算法，核心是将 RNN“展开”成前馈网络，再沿时间步反向计算梯度。
			
#### 1. BPTT 的基本步骤

1. 前向传播：按时间步计算 $h_1, h_2, \dots, h_T$ 和 $y_1, y_2, \dots, y_T$，并计算总损失 $L = \sum_{t=1}^T L_t$（$L_t$ 为第 $t$ 步的损失）；
2. 反向传播：从最后一个时间步 $T$ 开始，沿时间步反向计算各参数（$W_{xh}, W_{hh}, W_{hy}$ 等）的梯度，更新参数。

#### 2. BPTT 的核心：梯度计算

以循环权重 $W_{hh}$ 的梯度为例，其梯度依赖所有时间步的贡献：

$$
\frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^T \frac{\partial L}{\partial h_t} \cdot \frac{\partial h_t}{\partial W_{hh}}
$$

其中，$\frac{\partial h_t}{\partial h_{t-1}} = W_{hh}^T \cdot \sigma'(h_t)$（激活函数的导数），这会导致梯度中包含 $(W_{hh}^T \cdot \sigma')^{t-k}$ 项（$k$ 为历史时间步）。

---

### **五、基础 RNN 的致命问题：梯度消失与梯度爆炸**

BPTT 的梯度计算中，由于包含 $(W_{hh}^T \cdot \sigma')^{t-k}$ 项，当序列较长（$t$ 很大）时，会出现两个问题：

#### 1. 梯度消失（Vanishing Gradient）

若 $|W_{hh}^T \cdot \sigma'| < 1$，则 $(W_{hh}^T \cdot \sigma')^{t-k}$ 随时间步差（$t-k$）增大而指数级衰减，导致**早期时间步的梯度几乎为 0**（模型“记不住”长远的历史信息）。

- 示例：处理长句子“我昨天买了苹果，今天吃了____，它很甜”，基础 RNN 可能无法将“它”与“苹果”关联（因为“苹果”在太早的时间步，梯度消失导致 $W_{hh}$ 无法更新以捕捉这种长距离依赖）。

#### 2. 梯度爆炸（Exploding Gradient）

若 $|W_{hh}^T \cdot \sigma'| > 1$，则 $(W_{hh}^T \cdot \sigma')^{t-k}$ 随时间步差增大而指数级增长，导致梯度过大，参数更新时“跳过”最优解。

- 解决方法：梯度裁剪（Gradient Clipping）—— 当梯度超过阈值时，缩放梯度至阈值内（缓解爆炸，但无法解决消失）。

---

### **六、RNN 的变体：解决长距离依赖问题**

为克服梯度消失，研究者提出了改进的 RNN 变体，核心是通过“门控机制”控制信息的传递与遗忘，最经典的是**LSTM** 和 **GRU**：

#### 1. LSTM（长短期记忆网络，Long Short-Term Memory）

LSTM 通过 3 个门（输入门、遗忘门、输出门）和“细胞状态”（Cell State）实现长期记忆的保存与更新：

- 细胞状态：类似“传送带”，信息可长期传递（梯度不易消失）；
- 遗忘门：决定哪些历史信息应被遗忘；
- 输入门：决定哪些新信息应被存入细胞状态；
- 输出门：决定细胞状态中哪些信息应输出到隐藏状态。

#### 2. GRU（门控循环单元，Gated Recurrent Unit）

GRU 是 LSTM 的简化版，用 2 个门（更新门、重置门）合并了细胞状态和隐藏状态，参数更少，训练更快，在许多任务上性能接近 LSTM。

---

### **七、RNN 的应用场景**

尽管 LSTM/GRU 等变体更常用，但 RNN 的核心思想（循环记忆）仍是时序任务的基础，典型应用包括：

- 自然语言处理（NLP）：文本生成、机器翻译、情感分析、命名实体识别；
- 语音处理：语音识别、语音合成、声纹识别；
- 时序预测：股票价格预测、天气预测、传感器数据异常检测；
- 视频处理：视频分类、动作识别、帧标注。

---

### **总结**

RNN 通过**循环连接**和**隐藏状态记忆**，突破了前馈网络无法处理时序依赖的局限，是处理序列数据的里程碑模型。其核心优势是：

- 天然适配可变长度的时序输入；
- 能捕捉时间依赖关系（通过隐藏状态传递历史信息）；
- 参数共享，泛化能力强。

但基础 RNN 受限于梯度消失/爆炸，难以处理长序列，因此实际中多使用 LSTM/GRU 等变体。不过，理解基础 RNN 的结构与原理，是掌握所有循环神经网络的关键。