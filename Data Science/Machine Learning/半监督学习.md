半监督学习是一类机器学习方法，其核心是**利用少量标注数据（labeled data）和大量未标注数据（unlabeled data）进行模型训练**，以解决标注数据稀缺（标注成本高）但未标注数据易获取的问题。它介于监督学习（仅用标注数据）和无监督学习（仅用未标注数据）之间，通过挖掘未标注数据中的潜在结构信息，提升模型在标注数据上的泛化能力。


#### 一、核心假设：为什么未标注数据有效？
半监督学习的有效性依赖于对数据分布的基本假设，这些假设解释了“为什么未标注数据能辅助学习”：

1. **聚类假设（Cluster Assumption）**  
   同类数据会形成密集的聚类，未标注数据应与同聚类中的标注数据具有相同标签。例如，同一类别的图像（如“猫”）在特征空间中会聚集在一起，未标注数据的标签可通过其所在聚类的标注数据推断。

2. **流形假设（Manifold Assumption）**  
   高维数据实际分布在低维流形上，邻近的样本（在流形上距离近）具有相似的标签。例如，图像的像素是高维特征，但实际可通过低维的“形状”“纹理”等流形结构描述，相似纹理的图像更可能属于同一类。

3. **平滑假设（Smoothness Assumption）**  
   若两个样本在特征空间中距离近，则它们的标签也应相近。这是流形假设的简化，强调特征空间中局部区域的标签一致性。


#### 二、半监督学习的主要方法
根据原理不同，半监督学习可分为以下几类：


##### 1. 生成式方法（Generative Methods）
假设数据由某个**概率生成模型**生成，通过标注数据和未标注数据共同估计模型参数，再利用模型进行预测。核心是“用未标注数据优化生成模型的分布拟合”。

- **原理**：  
  假设数据的生成过程为$P(X, Y) = P(Y)P(X|Y)$，其中$Y$是标签，$X$是特征。  
  标注数据$X_l = \{(x_1, y_1), ..., (x_l, y_l)\}$和未标注数据$X_u = \{x_{l+1}, ..., x_{l+u}\}$的联合似然函数为：  
  $$L = \sum_{i=1}^l \log P(y_i, x_i) + \sum_{j=l+1}^{l+u} \log P(x_j)$$  
  其中$P(x_j) = \sum_{y} P(y)P(x_j|y)$（未标注数据的边缘概率）。通过最大化$L$估计参数（如$P(Y)$和$P(X|Y)$的参数），最终预测时用$P(Y|X) \propto P(Y)P(X|Y)$。

- **例子**：  
  半监督高斯混合模型（GMM）：假设数据由多个高斯分布混合生成，每个高斯分布对应一个类别。通过EM算法迭代估计高斯分布的均值、协方差和混合系数，利用未标注数据提升[[参数估计]]精度。[[聚类算法#高斯混合模型（GMM）]]


##### 2. 半监督支持向量机（Semi-Supervised SVM, S3VM）
在传统[[SVM]]基础上引入未标注数据，目标是找到一个超平面，不仅能正确分类标注数据，还能使未标注数据尽可能落在“最大间隔区域”内（即远离超平面）。

- **原理**：  
  传统SVM的目标是最小化$\frac{1}{2}||w||^2 + C\sum_{i=1}^l \xi_i$，约束为$y_i(w \cdot x_i + b) \geq 1 - \xi_i$（$\xi_i \geq 0$）。  
  S3VM加入对未标注数据$x_j$的约束：设未标注数据的预测标签为$\hat{y}_j \in \{+1, -1\}$，则需满足$\hat{y}_j(w \cdot x_j + b) \geq 1 - \xi_j$（$\xi_j \geq 0$），目标函数扩展为：  
  $$\min_{w, b, \xi, \hat{y}} \frac{1}{2}||w||^2 + C_l \sum_{i=1}^l \xi_i + C_u \sum_{j=l+1}^{l+u} \xi_j$$  
  其中$C_l$和$C_u$分别是标注和未标注数据的惩罚系数，需平衡两者的影响。

- **特点**：  
  核心是通过未标注数据“引导”超平面向数据分布稀疏的区域移动，最大化间隔的同时保持对未标注数据的“兼容性”。


##### 3. 基于图的方法（Graph-Based Methods）
将所有数据（标注+未标注）构建为图，节点为样本，边的权重表示样本间的相似度（如余弦相似度、高斯核$w_{ij} = \exp(-\frac{||x_i - x_j||^2}{2\sigma^2})$），再通过“标签传播”将标注数据的标签扩散到未标注数据。

- **原理**：  
  假设图上相邻节点的标签应相似，定义能量函数衡量标签的“不一致性”：  
  $$E(Y) = \frac{1}{2} \sum_{i,j} w_{ij}(Y_i - Y_j)^2$$  
  其中$Y_i$是节点$i$的标签（标注数据的$Y_i$固定，未标注数据的$Y_i$待求）。最小化$E(Y)$等价于让相似节点的标签尽可能一致。

  用拉普拉斯矩阵$L = D - W$（$D$是度矩阵，$D_{ii} = \sum_j w_{ij}$）表示，能量函数可写为$E(Y) = Y^T L Y$。通过约束标注数据的标签，求解最小化问题得到未标注数据的标签。

- **例子**：  
  标签传播（Label Propagation）：迭代更新未标注数据的标签为其邻居节点标签的加权平均，直至收敛。


##### 4. 自训练与协同训练（Self-Training & Co-Training）
这类方法属于“朴素半监督学习”，通过模型自身预测未标注数据的“伪标签”（pseudo-label），将高置信度的伪标签数据加入训练集迭代优化。

- **自训练（Self-Training）**：  
  1. 用标注数据训练初始模型$M$；  
  2. 用$M$预测未标注数据，选取预测置信度最高的样本（如概率$P(Y|X) > \theta$），赋予伪标签；  
  3. 将伪标签数据加入标注集，重新训练$M$；  
  4. 重复步骤2-3直至收敛。  

  核心是“用模型自身的预测扩展训练数据”，适用于任何可输出置信度的模型（如神经网络、决策树）。

- **协同训练（Co-Training）**：  
  假设数据存在两个“独立视图”（如文本的“词袋”和“主题”视图），满足“条件独立性”（给定标签时，两视图的特征独立）。  
  1. 用两个视图分别训练模型$M_1$和$M_2$；  
  2. $M_1$对未标注数据预测，选高置信度样本给$M_2$作为新增标注数据；  
  3. $M_2$同理，选高置信度样本给$M_1$；  
  4. 重复迭代，使两模型互相“教学”提升性能。  


##### 5. 基于深度学习的方法
随着深度学习发展，半监督学习与神经网络结合形成了一系列高效方法，核心是“利用未标注数据学习数据分布的先验知识”。

- **伪标签（Pseudo-Labeling）**：  
  类似自训练，在神经网络中，用训练好的模型对未标注数据预测，将概率最高的类别作为伪标签，训练时同时最小化标注数据的监督损失和伪标签数据的损失：  
  $$L = L_{sup}(X_l, Y_l) + \lambda L_{pseudo}(X_u, \hat{Y}_u)$$  
  其中$\hat{Y}_u$是伪标签，$\lambda$控制伪标签的权重。

- **一致性正则化（Consistency Regularization）**：  
  假设模型对“相似输入”应输出相似预测（流形假设的体现）。对未标注数据$x$进行微小扰动（如加噪声、数据增强、Dropout）得到$x'$，要求模型对$x$和$x'$的预测一致：  
  $$L_{consist} = ||P(Y|x) - P(Y|x')||_2^2$$  
  总损失为$L = L_{sup} + \lambda L_{consist}$。  

  代表方法：MixMatch（混合标注与未标注数据增强样本）、UDA（无监督数据增强）、FixMatch（结合伪标签和一致性正则化）。


#### 三、应用场景
半监督学习适用于“标注成本高，未标注数据丰富”的场景：  
- 文本分类：标注大量文本类别成本高，可用少量标注文本+海量未标注文本训练；  
- 图像识别：标注图像类别（如医学影像）耗时，可用少量标注图像+大量无标签图像提升模型鲁棒性；  
- 语音识别：语音转文字的标注成本高，利用未标注语音数据优化声学模型。


#### 四、挑战与局限
1. **假设依赖**：性能高度依赖聚类/流形假设，若数据分布不满足假设（如类别边界模糊），未标注数据可能降低模型性能；  
2. **伪标签噪声**：自训练等方法中，伪标签可能错误，导致“错误累积”；  
3. **计算复杂度**：基于图的方法需处理大规模图（$O(n^2)$复杂度），不适用于超大数据集；  
4. **多视图限制**：协同训练依赖“独立视图”，实际中难以满足。


#### 总结
半监督学习通过融合少量标注数据和大量未标注数据，在降低标注成本的同时提升模型性能，其核心是利用数据分布的先验假设（聚类、流形等）挖掘未标注数据的信息。从传统的生成式方法、图方法到现代深度学习中的伪标签和一致性正则化，半监督学习已成为解决“标注稀缺”问题的重要技术，在自然语言处理、计算机视觉等领域有广泛应用。