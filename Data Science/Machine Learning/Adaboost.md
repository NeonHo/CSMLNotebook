#### 1. 核心思想  
AdaBoost（Adaptive Boosting，自适应提升）是**Boosting集成学习**的经典算法，核心思想是：通过**迭代训练多个弱分类器**（性能略优于随机猜测，如决策桩、浅层决策树），并根据弱分类器的表现**动态调整样本权重**（让错误分类的样本在后续训练中被“重点关注”）和**弱分类器权重**（错误率越低的弱分类器权重越高），最终将所有弱分类器加权组合为一个强分类器。  


#### 2. 算法步骤  
假设训练集为$D = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$，其中$x_i$为样本特征，$y_i \in \{-1, +1\}$为二分类标签（多分类可扩展），共迭代$M$轮训练$M$个弱分类器。  


##### 步骤1：初始化样本权重  
第1轮训练前，所有样本权重均匀分布（每个样本被关注的程度相同）：  
$$w_{1i} = \frac{1}{N} \quad (i = 1, 2, ..., N)$$  
其中$w_{mi}$表示第$m$轮中第$i$个样本的权重，满足$\sum_{i=1}^N w_{mi} = 1$。  


##### 步骤2：训练第$m$个弱分类器  
在第$m$轮，基于当前样本权重$w_{mi}$训练弱分类器$h_m: \mathcal{X} \to \{-1, +1\}$，目标是最小化**加权错误率**：  
$$e_m = \sum_{i=1}^N w_{mi} \cdot I(h_m(x_i) \neq y_i)$$  
其中$I(\cdot)$为指示函数：若括号内条件为真，$I(\cdot)=1$；否则$I(\cdot)=0$。  

*注：弱分类器需满足$e_m < 0.5$（性能优于随机猜测），否则可将$h_m$反转（即$h_m' = -h_m$），此时错误率变为$1 - e_m < 0.5$。*  


##### 步骤3：计算弱分类器权重$\alpha_m$  
弱分类器的权重$\alpha_m$反映其“重要性”：错误率$e_m$越低，$\alpha_m$越大。公式为：  
$$\alpha_m = \frac{1}{2} \ln\left( \frac{1 - e_m}{e_m} \right)$$  

- 当$e_m \to 0$时，$\alpha_m \to +\infty$（几乎完美的弱分类器权重极大）；  
- 当$e_m = 0.5$时，$\alpha_m = 0$（随机猜测的分类器无贡献）。  


##### 步骤4：更新样本权重  
为让下一轮弱分类器更关注当前分类错误的样本，需调整样本权重：  
$$w_{(m+1)i} = \frac{w_{mi} \cdot \exp\left( -\alpha_m \cdot y_i \cdot h_m(x_i) \right)}{Z_m}$$  

其中：  
- $Z_m = \sum_{i=1}^N w_{mi} \cdot \exp\left( -\alpha_m \cdot y_i \cdot h_m(x_i) \right)$为**归一化因子**，确保$\sum_{i=1}^N w_{(m+1)i} = 1$；  
- 若$h_m(x_i) = y_i$（分类正确），则$y_i h_m(x_i) = 1$，权重更新为$w_{mi} \cdot \exp(-\alpha_m)$（降低权重）；  
- 若$h_m(x_i) \neq y_i$（分类错误），则$y_i h_m(x_i) = -1$，权重更新为$w_{mi} \cdot \exp(\alpha_m)$（升高权重）。  


##### 步骤5：构建强分类器  
经过$M$轮迭代后，将所有弱分类器加权组合，得到强分类器：  
$$H(x) = \text{sign}\left( \sum_{m=1}^M \alpha_m \cdot h_m(x) \right)$$  

其中$\text{sign}(\cdot)$为符号函数：输入为正输出$+1$，为负输出$-1$，本质是对弱分类器的“加权投票”。  
注意，符号函数的目的是让输出只能为两个数：+1和-1，提取了内部强分类器输出值的符号。

#### 3. 理论基础：指数损失最小化  
AdaBoost的迭代过程可通过**指数损失函数**解释其合理性。定义损失函数为：  
$$L(y, f(x)) = \exp(-y \cdot f(x))$$  

其中$f(x) = \sum_{m=1}^M \alpha_m h_m(x)$为强分类器的“分数输出”（未经过$\text{sign}$函数）。AdaBoost的每一步迭代，本质是在最小化当前损失函数：  

- 选择$h_m$最小化$\sum_{i=1}^N w_{mi} \exp(-y_i \alpha_m h_m(x_i))$（等价于最小化加权错误率$e_m$）；  
- 选择$\alpha_m$最小化$\sum_{i=1}^N w_{mi} \exp(-y_i \alpha_m h_m(x_i))$（推导后恰好得到$\alpha_m = \frac{1}{2} \ln\left( \frac{1 - e_m}{e_m} \right)$）；  
- 更新权重$w_{(m+1)i}$等价于损失函数的梯度下降步骤，使下一轮更关注高损失样本。  


#### 4. 特点与应用  
- **优点**：  
  - 无需手动设计弱分类器（常用决策桩），实现简单；  
  - 理论上可证明，若弱分类器错误率$e_m < 0.5$，则强分类器错误率随迭代次数增加呈指数下降；  
  - 对特征空间适应性强，可处理多种数据类型（如文本、图像）。  

- **缺点**：  
  - 对噪声和异常值敏感（异常值易被反复加权，导致过拟合）；  
  - 训练过程串行，无法并行加速（需依赖前一轮结果）。  

- **应用**：人脸识别（如Viola-Jones算法）、文本分类、生物特征识别等。  


综上，AdaBoost通过“自适应调整权重”和“加权组合弱分类器”，高效地将弱学习器提升为强学习器，是集成学习中兼具理论优美性和实践有效性的经典算法。