t-SNE（t-Distributed Stochastic Neighbor Embedding）是一种**非线性降维方法**，主要用于高维数据的可视化（如将100维特征映射到2D或3D空间）。它由Laurens van der Maaten和Geoffrey Hinton于2008年提出，核心思想是**保留高维数据中的局部相似性**，让高维空间中“近邻”的点在低维空间中依然“近邻”，同时缓解高维数据可视化中的“拥挤问题”（crowding problem）。


### 一、t-SNE的核心目标
高维数据（如图像、文本特征）直接可视化困难，t-SNE通过以下方式实现降维：  
- 衡量高维空间中数据点的“相似度”（即“近邻关系”）；  
- 在低维空间中构建相似的“近邻关系”；  
- 通过优化目标让低维空间的相似度分布尽可能接近高维空间，最终实现可视化。  


### 二、核心原理：从高维到低维的相似度映射
t-SNE的核心是**定义高维与低维空间的相似度分布**，并通过优化让两者尽可能接近。


#### 1. 高维空间的相似度：高斯分布建模
对于高维数据点$x_i$和$x_j$，t-SNE用**条件概率**定义它们的相似度：$p_{j|i}$表示“以$x_i$为中心时，$x_j$被选为近邻”的概率。  

具体计算如下：  
- 对每个$x_i$，假设其近邻服从**高斯分布**（正态分布），标准差为$\sigma_i$；  
- $p_{j|i}$的公式为：  
  $$p_{j|i} = \frac{\exp\left(-\frac{||x_i - x_j||^2}{2\sigma_i^2}\right)}{\sum_{k \neq i} \exp\left(-\frac{||x_i - x_k||^2}{2\sigma_i^2}\right)}$$  
  （其中$||x_i - x_j||^2$是$x_i$与$x_j$的欧氏距离平方，且$p_{i|i}=0$，即自身不视为近邻）。  


- **标准差$\sigma_i$的选择：困惑度（Perplexity）**  
  $\sigma_i$决定了高斯分布的“宽窄”，直接影响$p_{j|i}$的计算。t-SNE通过**困惑度（perplexity）** 来控制$\sigma_i$：  
  困惑度的定义为$Perp(P_i) = 2^{H(P_i)}$，其中$H(P_i) = -\sum_j p_{j|i} \log_2 p_{j|i}$是$p_{j|i}$的熵。  
  直观上，困惑度反映了“每个数据点的近邻数量”（类似“有效近邻数”），通常取值为5~50（常用30）。算法会通过二分法调整$\sigma_i$，使每个$x_i$的困惑度接近预设值。  


- **对称化处理**  
  为了让相似度具有对称性（$p_{ij}=p_{ji}$），t-SNE定义高维空间的联合概率：  
  $$p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$$  
  其中$n$是样本总数，目的是让所有$p_{ij}$的和为1（满足概率分布性质）。  


#### 2. 低维空间的相似度：t分布建模
设低维嵌入后的点为$y_i$（通常是2D或3D），t-SNE用**t分布（学生t分布，自由度为1，即柯西分布）** 定义低维空间的相似度。  

低维条件概率$q_{j|i}$的计算公式为：  
$$q_{j|i} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq i} (1 + ||y_i - y_k||^2)^{-1}}$$  
（$q_{i|i}=0$，且$||y_i - y_j||^2$是低维空间的欧氏距离平方）。  


- **为什么用t分布？**  
  高维空间中，高斯分布的尾部较“瘦”（远离中心的点概率极低）；而低维空间中，若仍用高斯分布，会导致“拥挤问题”——大量高维空间中距离较远的点在低维空间中被压缩成“拥挤的簇”。  
  t分布的尾部更“厚”（远离中心的点概率更高），能更好地保留高维空间中“较远但相关”的点的关系，缓解拥挤问题。  


- **对称化处理**  
  低维空间的联合概率同样定义为：  
  $$q_{ij} = \frac{q_{j|i} + q_{i|j}}{2n}$$  


#### 3. 损失函数：KL散度
t-SNE的目标是让低维分布$Q$（$q_{ij}$）尽可能接近高维分布$P$（$p_{ij}$），使用**KL散度（Kullback-Leibler divergence）** 衡量两者的差异：  
$$Loss = KL(P||Q) = \sum_{i < j} p_{ij} \log \frac{p_{ij}}{q_{ij}}$$  


- KL散度的特点：  
  - 非对称：$KL(P||Q) \neq KL(Q||P)$，更关注“高维中$p_{ij}$大的点对，在低维中$q_{ij}$也应大”（符合可视化需求：近邻关系优先）。  
  - 对$p_{ij}=0$的点对不敏感，主要优化高维空间中“确认为近邻”的点对。  


#### 4. 优化：梯度下降
t-SNE通过**梯度下降**最小化KL散度，求解低维嵌入$Y = \{y_1, y_2, ..., y_n\}$。  

对损失函数求关于$y_i$的梯度：  
$$\frac{\partial Loss}{\partial y_i} = 4 \sum_j (p_{ij} - q_{ij}) (y_i - y_j) (1 + ||y_i - y_j||^2)^{-1}$$  

直观理解：若$p_{ij} > q_{ij}$（高维中是近邻，但低维中不够近），梯度会推动$y_i$和$y_j$靠近；若$p_{ij} < q_{ij}$（高维中不是近邻，但低维中太近），梯度会推动两者远离。  


### 三、t-SNE的算法步骤
1. **输入高维数据**：$X = \{x_1, x_2, ..., x_n\}$（$x_i \in \mathbb{R}^d$，$d$为高维特征维度），预设低维维度（通常2或3）、困惑度、迭代次数。  
2. **计算高维联合概率$p_{ij}$**：  
   - 对每个$x_i$，通过困惑度确定$\sigma_i$，计算$p_{j|i}$；  
   - 对称化得到$p_{ij}$。  
3. **初始化低维嵌入**：$Y = \{y_1, y_2, ..., y_n\}$（通常随机初始化，值较小，如服从$N(0, 10^{-4})$分布）。  
4. **迭代优化**：  
   - 计算低维联合概率$q_{ij}$；  
   - 计算KL散度损失和梯度；  
   - 用梯度下降（带动量的SGD）更新$Y$（学习率通常50~200，迭代次数1000+）。  
5. **输出**：低维嵌入$Y$（可视化结果）。  


### 四、t-SNE的特点与参数
#### 核心特点
- **非线性**：能捕捉高维数据中的非线性结构（如流形、聚类），优于PCA等线性方法。  
- **局部优先**：更注重保留局部近邻关系，全局结构可能被扭曲（如距离、聚类大小不一定反映高维真实关系）。  
- **计算复杂度高**：$O(n^2)$（需计算所有点对的$p_{ij}$和$q_{ij}$），不适合百万级以上样本（可通过近似算法如 Barnes-Hut t-SNE 优化，复杂度降为$O(n \log n)$）。  


#### 关键参数
- **困惑度（Perplexity）**：影响近邻数量，过小会导致聚类过碎，过大会模糊局部结构（常用30）。  
- **迭代次数**：通常需要1000~5000次，迭代不足会导致聚类未稳定，过多可能过度优化。  
- **学习率**：控制更新步长，过小收敛慢，过大会震荡（常用200）。  


### 五、应用与对比
#### 典型应用
- 高维数据可视化：如MNIST手写数字（784维→2D）、基因表达数据（数千维→2D）、图像特征（如CNN输出→2D）。  
- 聚类效果验证：通过低维分布直观判断高维数据是否存在自然聚类。  


#### 与其他方法的对比
| 方法       | 类型       | 特点                                                                 | 适用场景                     |
|------------|------------|----------------------------------------------------------------------|------------------------------|
| t-SNE      | 非线性     | 保留局部结构，可视化效果好，计算慢，不保留全局距离                     | 高维数据可视化               |
| PCA        | 线性       | 保留全局方差最大方向，计算快，无法捕捉非线性结构                       | 线性可分数据降维/去噪        |
| UMAP       | 非线性     | 保留局部+部分全局结构，计算快（$O(n \log n)$），适合大规模数据         | 快速可视化、下游任务降维     |
| Isomap     | 非线性     | 保留全局 geodesic 距离，对噪声敏感，计算慢                             | 流形结构明显的数据           |  


### 六、注意事项
1. t-SNE的结果是**相对的**：低维空间中聚类的距离/大小不代表高维真实关系，仅反映“是否为近邻”。  
2. 对参数敏感：不同困惑度/迭代次数可能得到差异较大的结果，需多次尝试。  
3. 不适合作为下游任务的降维（如分类）：因其不保证映射的一致性（新样本无法直接嵌入），且损失函数仅为可视化设计。  


### 总结
t-SNE通过高斯分布和t分布分别建模高维与低维空间的相似度，以KL散度为优化目标，最终实现高维数据的有效可视化。它的核心优势是**保留局部近邻关系**，是目前高维数据可视化的主流方法之一，但需注意其结果的解读限制和参数敏感性。