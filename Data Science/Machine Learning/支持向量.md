
支持向量（Support Vector）是支持向量机（[[SVM]]）中的核心概念，指的是**训练数据中位于分类边界（间隔边界）上或被错误分类的样本点**。这些样本直接决定了 SVM 的最优分类超平面，而其他样本对超平面的位置没有影响。以下是其详细解释：

### 1. 直观理解
在二分类问题中，SVM 的目标是找到一个超平面，使得不同类别的样本被最大的“间隔”分开。支持向量就是**恰好位于这个间隔边界上的样本点**，或者是**被错误分类的样本点**（在软间隔 SVM 中）。

- **硬间隔 SVM**：支持向量是恰好位于间隔边界上的样本（即满足 $y_i(w^T x_i + b) = 1$ 的样本）。
- **软间隔 SVM**：支持向量包括间隔边界上的样本、间隔内部的样本（但被正确分类），以及错误分类的样本（即满足 $y_i(w^T x_i + b) \leq 1$ 的样本）。

### 2. 数学定义
对于线性可分的二分类问题（标签 $y \in \{-1, 1\}$），SVM 的最优超平面由以下条件确定：

$$
\min_{w, b} \frac{1}{2} \|w\|^2 \quad \text{s.t.} \quad y_i(w^T x_i + b) \geq 1 \quad \forall i
$$

其中，**支持向量**是指满足以下条件的样本 $x_i$：

- **硬间隔**：$y_i(w^T x_i + b) = 1$（即位于间隔边界上的样本）。
- **软间隔**：引入松弛变量 $\xi_i \geq 0$ 后，支持向量满足 $y_i(w^T x_i + b) \geq 1 - \xi_i$，且 $\xi_i > 0$（即间隔内部或被错误分类的样本）。

### 3. 支持向量的作用
- **决定分类超平面**：SVM 的最优超平面仅由支持向量决定，其他样本不影响超平面的位置和方向。
- **稀疏性**：训练完成后，大多数样本对预测无贡献，模型仅需存储支持向量，节省内存和计算资源。
- **泛化能力**：通过最大化间隔，SVM 隐式地降低了模型复杂度，减少过拟合风险，提高对未知数据的泛化能力。

### 4. 核函数与非线性支持向量
在非线性 SVM 中，数据通过核函数映射到高维空间后线性可分。此时，支持向量是**原始空间中映射到高维后位于间隔边界上的样本**。核函数允许 SVM 隐式处理高维特征，而无需显式计算映射后的坐标。

例如，对于高斯核 $K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$，支持向量在预测时通过核函数与新样本交互，实现非线性分类。

### 5. 与其他分类器的对比
| 模型               | 决策边界依赖的样本               | 存储需求               | 对异常值的敏感性 |
|--------------------|----------------------------------|------------------------|------------------|
| **SVM（支持向量）** | 仅间隔边界上或错误分类的样本     | 低（仅需存储支持向量） | 低               |
| 逻辑回归           | 所有样本（通过梯度更新参数）     | 高（需存储所有样本）   | 高               |
| 决策树             | 所有样本（通过信息增益划分）     | 高                     | 中               |

### 6. 代码示例（可视化支持向量）
以下代码使用 Python 和 scikit-learn 训练 SVM，并可视化支持向量：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC

# 加载鸢尾花数据集（取前两个特征）
iris = datasets.load_iris()
X = iris.data[:, :2]  # 仅取前两个特征
y = iris.target
y = np.where(y == 0, -1, 1)  # 二分类：类别0 vs 类别1+2

# 训练SVM（线性核）
clf = SVC(kernel='linear', C=1000)  # 大C值近似硬间隔
clf.fit(X, y)

# 获取支持向量
support_vectors = clf.support_vectors_

# 绘制数据点和决策边界
plt.figure(figsize=(10, 6))
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='b', label='类别1')
plt.scatter(X[y == -1][:, 0], X[y == -1][:, 1], c='r', label='类别-1')

# 绘制支持向量
plt.scatter(support_vectors[:, 0], support_vectors[:, 1],
            s=100, facecolors='none', edgecolors='k', label='支持向量')

# 绘制决策边界和间隔边界
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# 创建网格点
xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = clf.decision_function(xy).reshape(XX.shape)

# 绘制决策边界和间隔边界
ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,
           linestyles=['--', '-', '--'])

plt.xlabel('特征1')
plt.ylabel('特征2')
plt.title('SVM 分类器与支持向量')
plt.legend()
plt.show()
```

运行上述代码后，图中黑色边框的点即为支持向量，它们位于决策边界的间隔上。

### 总结
支持向量是 SVM 中最关键的样本点，它们决定了分类超平面的位置和方向。通过仅关注这些边界样本，SVM 实现了高效的模型表示和强大的泛化能力，尤其适合处理高维数据和小样本问题。理解支持向量的概念是掌握 SVM 工作原理的核心。
