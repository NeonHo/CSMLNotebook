是的，[[交叉熵损失]]（Cross-Entropy Loss）本质上是**KL 散度（Kullback-Leibler Divergence）的简化形式**，二者在分类任务中存在紧密的数学联系，但目标侧重不同。

### KL 散度、交叉熵、交叉熵损失的关系

首先明确三个概念的数学定义（以单样本多分类为例）：  
- 设真实标签为 **one-hot 向量**[[One-Hot编码]] $y$（仅正确类别为 1，其余为 0），模型预测概率分布为 $\hat{y}$（通过 softmax 等函数归一化的概率）。

#### 1. KL 散度（相对熵）
衡量两个概率分布 $y$ 和 $\hat{y}$ 的“差异”，公式为：  
$$
\text{KL}(y \parallel \hat{y}) = \sum_{i} y_i \log\left(\frac{y_i}{\hat{y}_i}\right)
$$  
由于真实标签 $y$ 是 one-hot 向量（只有正确类别 $c$ 的 $y_c=1$，其余为 0），简化为：  
$$
\text{KL}(y \parallel \hat{y}) = -\log(\hat{y}_c) - \sum_i y_i \log(y_i)
$$

#### 2. 交叉熵（Cross-Entropy）

![[信息熵与信息增益#1. 信息熵：混乱程度温度计]]

定义为  
$$
\text{CE}(y, \hat{y}) = -\sum_i y_i \log(\hat{y}_i)
$$  
结合 one-hot 标签的特性，简化为：  
$$
\text{CE}(y, \hat{y}) = -\log(\hat{y}_c)
$$  
（仅保留正确类别 $c$ 的预测概率的负对数）

#### 3. 交叉熵损失与 KL 散度的关系
对比 KL 散度和交叉熵的公式可知：  
$$
\text{KL}(y \parallel \hat{y}) = \text{CE}(y, \hat{y}) - H(y)
$$  
其中 $H(y) = -\sum_i y_i \log(y_i)$ 是真实标签分布的熵（对于 one-hot 标签，$H(y)=0$，因为 $y_i$ 非 0 即 1）。

因此，在分类任务中（one-hot 标签），**KL 散度等价于交叉熵损失**。交叉熵损失本质上是 KL 散度的简化形式——当真实分布固定时，最小化交叉熵即等价于最小化 KL 散度。

---

### 为什么用交叉熵损失而非直接用 KL 散度？

1. **计算简化**  
   对于 one-hot 标签，真实分布的熵 $H(y)=0$，交叉熵直接等于 KL 散度，无需额外计算 $H(y)$，更高效。

2. **目标一致**  
   优化的核心是让预测分布接近真实分布，KL 散度和交叉熵在分类任务中目标完全一致（因 $H(y)$ 是常数），用交叉熵更简洁。

---

### 总结

交叉熵损失是 KL 散度在 **分类任务（one-hot 标签）** 下的简化形式，二者本质上等价——都是通过衡量“预测分布与真实分布的差异”来优化模型。交叉熵损失的广泛使用，是因为它在保持 KL 散度优化目标的同时，计算更简洁，更适配分类任务的特性。