独立成分分析（Independent Component Analysis，ICA）是一种基于**统计独立性**的信号处理与降维方法，核心目标是从多个观测信号中分离出潜在的、统计独立的源信号。

与[[PCA]]（主成分分析）不同，ICA不仅关注数据的二阶统计特性（如相关性），更强调高阶统计特性（如独立性），因此在非高斯信号分离和特征提取中具有独特优势。


#### 一、ICA的核心思想与基本假设
ICA的核心思想源于“盲源分离”问题：假设观测数据是多个独立源信号的线性混合，且源信号和混合过程均未知，ICA通过分析观测数据的统计特性，反推出混合机制并分离出独立的源信号，进而实现数据降维（保留关键独立成分，舍弃冗余信息）。

**基本假设**（ICA有效的前提）：
1. **源信号独立性**：潜在的源信号彼此统计独立，即对于任意两个源信号$s_i$和$s_j$（$i \neq j$），它们的联合概率密度等于边缘概率密度的乘积：  
   $p(s_i, s_j) = p(s_i) \cdot p(s_j)$。  
2. **混合矩阵满秩**：混合过程是线性的，且混合矩阵$A$为满秩矩阵（保证源信号可分离）。  
3. **非高斯性**：源信号中最多只有一个是高斯分布。（原因：高斯信号的高阶统计量对称，无法通过独立性区分；非高斯信号的高阶统计量可用于度量独立性）。  


#### 二、ICA的数学模型
设观测数据为$X = [x_1, x_2, ..., x_n]^T$（$n$个观测变量），潜在的源信号为$S = [s_1, s_2, ..., s_m]^T$（$m$个独立源，通常$m \leq n$），则ICA的线性混合模型为：  
$$X = A \cdot S$$  
其中，$A$为$n \times m$的**混合矩阵**（未知），描述源信号如何线性组合成观测信号。

ICA的目标是估计一个$m \times n$的**分离矩阵**$W$，使得分离后的信号$Y = W \cdot X$尽可能接近源信号$S$，即$Y \approx S$。此时，$W$可视为$A$的逆（或近似逆），即$W \approx A^{-1}$（当$n = m$且$A$可逆时）。


#### 三、独立性的度量：从“不相关”到“独立”
独立性是ICA的核心，但“独立”比“不相关”（PCA的核心）更严格：  
- **不相关**：仅要求变量的二阶矩（协方差）为0，即$E[xy] = E[x]E[y]$；  
- **独立**：要求变量的所有阶矩都满足联合分布等于边缘分布的乘积，即$p(x,y) = p(x)p(y)$，这是更强的条件。

为量化独立性，ICA需通过**高阶统计量**（而非仅协方差）度量，常用指标包括：

1. **峰度（Kurtosis）**  
   峰度是描述信号分布“尖峭程度”的四阶统计量，定义为：  
   $Kurt(y) = E[y^4] - 3(E[y^2])^2$  
   - 高斯信号的峰度为0（因三阶矩为0，四阶矩满足$E[y^4] = 3(E[y^2])^2$）；  
   - 非高斯信号的峰度非零（如脉冲信号峰度为正，均匀信号峰度为负）。  
   因此，峰度可用于衡量信号的“非高斯性”，非高斯性越强，越容易通过ICA分离。


1. **负熵（Negentropy）**  [[信息熵与信息增益#1. 信息熵：混乱程度温度计]]
   熵是衡量信号不确定性的指标，高斯信号在固定方差下熵最大。负熵定义为：  
   $J(y) = H(y_{\text{gauss}}) - H(y)$  
   其中，$H(y)$为信号$y$的熵，$y_{\text{gauss}}$是与$y$具有相同均值和方差的高斯信号。  
   负熵越大，信号的非高斯性越强（独立性越易度量），是ICA中常用的优化目标。


2. **互信息（Mutual Information）**  [[互信息]]
   互信息衡量变量间的依赖关系，定义为：  
   $I(Y) = \sum_{i=1}^m H(y_i) - H(Y)$  
   其中$H(Y)$是联合熵，$H(y_i)$是边缘熵。若$Y$的各分量独立，则$I(Y) = 0$。因此，ICA可通过**最小化互信息**实现信号分离。


#### 四、ICA的预处理：中心化与白化
为简化计算，ICA需对观测数据进行预处理，核心步骤包括：

1. **中心化**  
   移除数据的均值：$x_i' = x_i - E[x_i]$，使$E[X] = 0$（源信号和观测信号的均值可假设为0，不影响独立性）。

2. **白化（Whitening）**  
   白化（或球化）使数据的协方差矩阵为单位矩阵：$E[X_{\text{white}} X_{\text{white}}^T] = I$。  
   作用：去除数据的二阶相关性（简化混合矩阵为正交矩阵），使ICA专注于高阶统计量的独立性。  
   实现：通过PCA完成，即$X_{\text{white}} = V^T (X - \mu)$，其中$V$是协方差矩阵特征向量组成的矩阵，$\mu$是均值。


#### 五、常用ICA算法：FastICA
FastICA（快速独立成分分析）是最流行的ICA算法，基于**固定点迭代**最大化信号的非高斯性（以负熵为目标），步骤如下：

1. **预处理**：对观测数据$X$进行中心化和白化，得到$X_{\text{white}}$。  
2. **初始化**：随机选择一个单位向量$w$（分离矩阵的一列）。  
3. **迭代优化**：通过固定点迭代更新$w$，目标是最大化$E[g(w^T X_{\text{white}})]$（$g$是非线性函数，如$g(u) = \tanh(a u)$或$g(u) = u e^{-u^2/2}$，用于增强非高斯性）。  
   迭代公式：$w \leftarrow E[X_{\text{white}} g(w^T X_{\text{white}})] - E[g'(w^T X_{\text{white}})] w$，其中$g'$是$g$的导数。  
4. **归一化**：每次迭代后将$w$归一化（$w \leftarrow w / \|w\|$）。  
5. **收敛判断**：若$w$的更新量小于阈值（如$10^{-4}$），则停止迭代。  
6. **多成分扩展**：若需分离$m$个源信号，重复步骤2-5得到$m$个向量，并用Gram-Schmidt正交化保证各向量正交（避免提取重复成分）。  


#### 六、ICA与PCA的对比
| 特性         | PCA（主成分分析）               | ICA（独立成分分析）             |
|--------------|----------------------------------|----------------------------------|
| 核心目标     | 最大化方差（保留主要信息）       | 最大化独立性（分离独立源）       |
| 统计特性     | 利用二阶统计量（协方差、相关性） | 利用高阶统计量（峰度、独立性）   |
| 结果性质     | 主成分不相关                     | 独立成分统计独立                 |
| 适用场景     | 数据压缩、去噪（高斯/非高斯数据） | 盲源分离、特征提取（非高斯数据） |
| 输出顺序     | 按方差降序排列                   | 无固定顺序（需结合业务解释）     |


#### 七、ICA的应用场景
1. **盲源分离**：如“鸡尾酒会问题”（从混合录音中分离多个说话人的声音）、图像处理（分离图像中的独立纹理）。  
2. **特征降维**：在模式识别中提取独立特征（如人脸识别中分离光照、姿态等独立因素）。  
3. **生物信号分析**：分离脑电信号（EEG）、心电信号（ECG）中的干扰成分（如肌电、工频噪声）。  
4. **金融数据处理**：从混合的股票价格中分离独立的市场驱动因素（如政策、行业因素）。  


#### 八、ICA的局限性
1. **依赖非高斯性**：若源信号多为高斯分布，ICA无法有效分离（高斯信号的高阶统计量无法区分）。  
2. **线性混合假设**：仅适用于线性混合模型，对非线性混合效果差。  
3. **源数量限制**：观测信号数量需不少于源信号数量（$n \geq m$）。  
4. **顺序不确定性**：分离出的独立成分顺序随机，需结合先验知识解释。  


### 总结
ICA通过挖掘数据的高阶统计独立性，实现了从混合信号中分离独立源的目标，是降维和盲源分离的强大工具。其核心在于利用非高斯性度量（如负熵）和FastICA等高效算法，在信号处理、机器学习等领域具有广泛应用。与PCA相比，ICA更适合处理非高斯、具有独立潜在因素的数据，为复杂数据的特征提取提供了新思路。