[[Autoencoder]]

变分自编码器（Variational Autoencoder, VAE）是一种融合**自编码器结构**与**变分推断理论**的生成模型，核心目标是学习数据的潜在概率分布，从而实现新数据的生成。与传统自编码器（AE）相比，VAE通过对潜在空间施加概率约束，解决了AE潜在空间不连续、难以采样生成新数据的问题，成为生成式建模的重要范式。

![[Pasted image 20250720172811.png]]
### 一、VAE的核心动机：从“确定性编码”到“概率生成”
传统自编码器（AE）由编码器（Encoder）和解码器（Decoder）组成：编码器将输入数据$x$映射到确定性潜在向量$z$，解码器再将$z$重构为$\hat{x}$，通过最小化重构损失（如MSE）学习数据的压缩表示。但传统AE存在一个关键缺陷：**潜在空间缺乏连续性和结构性**——编码器输出的$z$可能在潜在空间中离散分布，导致从潜在空间随机采样的向量经解码器生成的结果无意义（例如，两张人脸图像的潜在向量之间的插值可能生成扭曲的图像）。

VAE的创新在于：将潜在向量$z$建模为**概率分布**（而非确定值），通过约束潜在分布的形式，使潜在空间具有连续性和可采样性，从而实现“从潜在分布采样→生成新数据”的能力。


### 二、VAE的结构与核心原理
VAE的结构仍由编码器和解码器组成，但两者的功能与传统AE有本质区别：

In VAEs, 
the encoder still maps the input data to a lower-dimensional latent space, but instead of a single point in the latent space, the encoder generates a probability distribution over the latent space. 
The decoder then samples from this distribution to generate a new data point. This probabilistic approach to encoding the input allows VAEs to learn a more structured and continuous latent space representation, which is useful for generative modeling and data synthesis.
#### 1. 编码器（Inference Network）：学习潜在分布的参数
编码器的目标不是输出确定性的$z$，而是输出**潜在分布$p(z|x)$的参数**。假设潜在分布$p(z|x)$服从高斯分布（这是变分推断中常见的简化，便于计算），则编码器对输入$x$进行编码后，输出两个向量：
- 均值向量$\mu(x)$：表示高斯分布的均值；
- 标准差向量$\sigma(x)$：表示高斯分布的标准差（通常输出$\log \sigma^2(x)$，避免标准差为负）。

即：编码器将$x$映射到$z \sim \mathcal{N}(\mu(x), \sigma^2(x)I)$，其中$I$是单位矩阵（假设各维度独立）。


#### 2. 解码器（Generative Network）：从潜在分布生成数据
解码器的目标是学习**条件分布$p(x|z)$**：给定潜在变量$z$，解码器输出$x$的概率分布参数（例如，若$x$是图像，可假设$p(x|z)$为[[伯努利分布]]，输出每个像素的概率；或高斯分布，输出均值和标准差）。

通过解码器，我们可以从潜在空间采样$z$，再生成新数据$x \sim p(x|z)$。


#### 3. 核心：变分推断与损失函数
VAE的训练目标是最大化**边际似然**$p(x) = \int p(x|z)p(z)dz$（即数据$x$由模型生成的概率），但该积分难以直接计算。VAE通过**变分推断**，用一个可计算的近似分布$q(z|x)$（即编码器输出的分布）逼近真实后验$p(z|x)$，将目标转化为最大化**证据下界（Evidence Lower Bound, ELBO）**。

ELBO的表达式为：  
$$\text{ELBO} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - \text{KL}(q(z|x) \parallel p(z))$$  

其中包含两部分：
- **重构损失（Reconstruction Loss）**：$\mathbb{E}_{q(z|x)}[\log p(x|z)]$  
  衡量解码器从$z$重构$x$的能力，即生成数据与输入数据的相似度。例如，若$x$是灰度图像（像素值0-1），可采用二进制[[交叉熵损失]]（BCE）；若为连续值，可采用均方误差（MSE）。

- **KL散度正则项（KL Regularization）**：$\text{KL}(q(z|x) \parallel p(z))$  [[交叉熵损失&KL散度#1. KL 散度（相对熵）]]
  衡量编码器输出的分布$q(z|x)$与先验分布$p(z)$的差异。VAE通常假设先验$p(z)$为标准正态分布$\mathcal{N}(0, I)$，这一正则项强制$q(z|x)$接近$p(z)$，确保潜在空间连续且结构良好（便于采样生成新数据）。

  当$q(z|x) \sim \mathcal{N}(\mu, \sigma^2I)$且$p(z) \sim \mathcal{N}(0, I)$时，KL散度可解析计算为：  
  $$\text{KL} = \frac{1}{2} \sum_{i=1}^d (\mu_i^2 + \sigma_i^2 - \log \sigma_i^2 - 1)$$  
  （$d$为潜在空间维度）


#### 4. 重参数化技巧（Reparameterization Trick）
由于$z$是从$q(z|x)$采样得到的，而采样过程不可导（梯度无法通过随机节点传播），VAE通过**重参数化**解决这一问题：  
将$z$表示为$z = \mu(x) + \sigma(x) \cdot \epsilon$，其中$\epsilon \sim \mathcal{N}(0, I)$（从标准正态分布采样的噪声）。  

这样，$z$的随机性转移到$\epsilon$上，而$\mu(x)$和$\sigma(x)$是编码器的确定性输出，梯度可通过$\mu$和$\sigma$反向传播，实现端到端训练。


### 三、VAE的工作流程
1. **训练阶段**：  
   - 输入数据$x$，编码器输出$\mu(x)$和$\sigma(x)$；  
   - 用重参数化技巧采样$z = \mu + \sigma \cdot \epsilon$；  
   - 解码器从$z$生成$\hat{x}$，计算重构损失；  
   - 计算$q(z|x)$与先验$p(z)$的KL散度；  
   - 最小化“-ELBO”（即最大化ELBO），更新编码器和解码器参数。

2. **生成阶段**：  
   - 从先验分布$p(z) \sim \mathcal{N}(0, I)$采样$z$；  
   - 解码器将$z$映射为新数据$\hat{x} \sim p(x|z)$，完成生成。


### 四、VAE与其他模型的对比
| 模型      | 核心思想             | 潜在空间特点       | 生成能力        | 训练稳定性     |
| ------- | ---------------- | ------------ | ----------- | --------- |
| 传统自编码器  | 确定性编码-解码，最小化重构损失 | 不连续、不可控      | 难以生成新数据     | 高         |
| VAE     | 概率编码-解码，最大化ELBO  | 连续、结构化（接近先验） | 可生成新数据（较模糊） | 高（基于概率模型） |
| [[GAN]] | 对抗训练，生成分布逼近真实分布  | 无显式约束（依赖训练）  | 生成质量高（清晰）   | 低（易模式坍塌）  |


### 五、VAE的优缺点
#### 优点：
- **理论基础扎实**：基于变分推断，生成过程有明确的概率解释；  
- **训练稳定**：损失函数可稳定优化，无GAN的对抗失衡问题；  
- **潜在空间连续**：KL散度约束使潜在空间平滑，便于插值生成（例如，在两个图像的潜在向量之间插值，生成过渡图像）。

#### 缺点：
- **生成质量略逊**：重构损失倾向于“平均化”误差，生成样本可能模糊（相比GAN的清晰细节）；  
- **依赖先验假设**：假设潜在分布为高斯分布，可能限制对复杂数据的建模能力；  
- 计算成本较高：需要采样多个$z$计算期望（实际中常用单次采样近似）。


### 六、应用场景
VAE在生成式建模和表示学习中应用广泛：  
- **图像生成**：生成人脸、手写数字等（如MNIST、CelebA数据集）；  
- **异常检测**：通过重构误差识别与训练数据分布差异大的异常样本；  
- **数据增强**：生成新样本扩充数据集；  
- **潜在空间插值**：实现图像风格、属性的平滑过渡（如人脸年龄变化、表情转换）。


### 总结
变分自编码器（VAE）通过将自编码器与变分推断结合，首次实现了“从潜在概率分布采样生成新数据”的能力。其核心创新是用概率分布建模潜在空间，并通过ELBO损失和重参数化技巧解决了训练中的技术难题。尽管生成质量不如GAN，但VAE的稳定性和概率解释使其成为生成式建模的重要工具，尤其在需要结构化潜在空间的场景中（如插值生成、表示学习）表现突出。