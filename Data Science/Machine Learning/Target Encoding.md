Target Encoding（目标编码）是一种强大的类别特征编码技术，核心思想是利用类别特征与目标变量（标签）的统计关系，将类别值转换为基于目标变量的量化值。它在处理高基数特征（类别数量多）时表现优异，且能为模型提供更具预测价值的信息。以下从原理、变种、实现、问题与解决方案等方面详细解析：


### 一、核心原理
Target Encoding 的本质是用**类别对应的目标变量统计量**作为该类别的编码值。具体来说，对于每个类别 $c$，其编码值由该类别在训练集中对应的目标变量 $y$ 的统计特征（如均值、中位数、众数等）决定。

#### 1. 分类任务（以二分类为例）
假设目标变量 $y \in \{0, 1\}$，对于类别 $c$，编码值通常为该类别出现时 $y=1$ 的概率（均值）：  
$$\text{encode}(c) = \frac{\sum_{i: x_i = c} y_i}{n_c}$$  
其中 $n_c$ 是类别 $c$ 在训练集中的样本数量，$\sum_{i: x_i = c} y_i$ 是该类别中目标为 1 的样本数。

**示例**：  
特征 `City` 取值为 `["北京", "上海", "北京", "广州", "上海"]`，对应的目标变量 `y` 为 `[1, 1, 0, 0, 1]`。  
- 北京：$n_c=2$，$\sum y_i=1$ → 编码值 $=1/2=0.5$  
- 上海：$n_c=2$，$\sum y_i=2$ → 编码值 $=2/2=1.0$  
- 广州：$n_c=1$，$\sum y_i=0$ → 编码值 $=0/1=0.0$  

最终编码结果：`["北京", "上海", "北京", "广州", "上海"]` → `[0.5, 1.0, 0.5, 0.0, 1.0]`。


#### 2. 回归任务
对于回归任务（目标变量 $y$ 为连续值），编码值通常为类别 $c$ 对应的 $y$ 的均值：  
$$\text{encode}(c) = \frac{\sum_{i: x_i = c} y_i}{n_c}$$  

**示例**：  
特征 `户型` 取值为 `["一居室", "两居室", "一居室"]`，目标变量 `房价` 为 `[50, 80, 60]`。  
- 一居室：均值 $=(50+60)/2=55$ → 编码值 $55$  
- 两居室：均值 $=80/1=80$ → 编码值 $80$  


### 二、关键变种
基础的目标编码（直接用均值）容易因小样本类别导致过拟合，因此衍生出多种改进版本：

#### 1. 平滑目标编码（Smoothing）
为解决小样本类别的统计值不可靠问题，引入平滑机制，平衡类别内均值与全局均值：  
$$\text{encode}(c) = \frac{n_c \cdot \mu_c + m \cdot \mu_{\text{global}}}{n_c + m}$$  
其中：  
- $n_c$：类别 $c$ 的样本量；  
- $\mu_c$：类别 $c$ 的目标均值；  
- $\mu_{\text{global}}$：所有样本的目标均值（全局均值）；  
- $m$：平滑参数（控制全局均值的权重，$m$ 越大，平滑越强）。  

**示例**：若 $n_c=2$（小样本），$\mu_c=1.0$，$\mu_{\text{global}}=0.5$，$m=10$，则编码值 $=(2×1.0 + 10×0.5)/(2+10)=7/12≈0.58$，更接近全局均值，避免小样本偏差。


#### 2. 留一法编码（Leave-One-Out Encoding）
当类别 $c$ 包含当前样本时，编码值排除当前样本的目标值，避免“用自身预测自身”的泄露：  
$$\text{encode}(c, i) = \frac{\sum_{j: x_j = c, j \neq i} y_j}{n_c - 1}$$  

**适用场景**：小数据集（样本量少，单个样本对均值影响大）。


#### 3. 交叉验证编码（Cross-Validation Encoding）
将训练集分为 $K$ 折，用第 $k$ 折之外的所有数据计算编码值，再用于第 $k$ 折的训练，完全避免当前折的目标信息泄露：  
- 第 1 折的编码值 = 基于第 2~K 折的统计量；  
- 第 2 折的编码值 = 基于第 1,3~K 折的统计量；  
- ... 以此类推。  

**优势**：彻底解决数据泄露，是工业界常用的稳健方法。


#### 4. 贝叶斯目标编码（Bayesian Target Encoding）
基于贝叶斯估计，将类别均值视为先验分布（如正态分布）的后验期望，进一步稳定小样本类别的编码值。


### 三、实现步骤（以二分类为例）
1. **计算全局目标均值**：$\mu_{\text{global}} = \frac{\sum_{i=1}^N y_i}{N}$（$N$ 为总样本数）。  
2. **按类别分组统计**：对每个类别 $c$，计算样本量 $n_c$ 和类别内目标均值 $\mu_c$。  
3. **应用平滑（可选）**：若需平滑，用公式 $\text{encode}(c) = \frac{n_c \cdot \mu_c + m \cdot \mu_{\text{global}}}{n_c + m}$ 计算编码值。  
4. **处理测试集**：用训练集的统计量（而非测试集）编码测试集，避免数据泄露。  
5. **交叉验证编码（可选）**：若担心泄露，用 K 折交叉验证重新计算训练集编码值。  


### 四、优点与适用场景
#### 优点
- **保留预测信息**：编码值直接反映类别与目标的关联，对模型更具参考价值。  
- **避免维度爆炸**：无论类别数量多少，编码后仍为 1 维特征，适合高基数特征（如城市、用户 ID）。  
- **适配线性模型**：线性模型（如逻辑回归）对数值敏感，目标编码能有效传递类别与目标的关系。  

#### 适用场景
- 高基数特征（类别数 $>10$，如职业、地区）：优于 One-Hot 编码（不会导致维度灾难）。  
- 线性模型或深度学习：树模型对编码方式较不敏感，而线性模型依赖特征的数值关联性。  
- 样本量较大的数据集：能更稳定地估计类别与目标的统计关系。  


### 五、风险与解决方案
#### 主要风险
1. **数据泄露**：若直接用训练集的统计量编码训练集，可能导致模型“记住”训练集的目标信息（尤其是小样本类别），泛化能力下降。  
2. **过拟合小样本类别**：样本量极少的类别（如 $n_c=1$），其目标均值波动极大，易被模型当作“强特征”。  
3. **类别不平衡影响**：若某个类别占比极高（如 90%），其编码值可能主导模型学习，忽略其他类别。  


#### 解决方案
- **交叉验证编码**：用 K 折交叉验证计算训练集编码值，确保每个样本的编码值不依赖自身的目标值。  
- **最小样本量过滤**：对 $n_c < t$（$t$ 为阈值，如 5）的类别，直接用全局均值编码。  
- **添加噪声**：对编码值加入微小随机噪声（如 $\epsilon \sim \mathcal{N}(0, \sigma^2)$，$\sigma$ 很小），减少模型对高频类别的过度依赖。  
- **调整平滑参数**：增大 $m$ 可增强平滑效果，降低小样本类别的影响（需通过交叉验证选择最优 $m$）。  


### 六、与其他编码方法的对比
| 编码方法               | 核心逻辑       | 高基数特征适配性   | 数据泄露风险 |
| ------------------ | ---------- | ---------- | ------ |
| Target Encoding    | 用目标变量统计值编码 | 优秀         | 高（需处理） |
| [[Label Encoding]] | 用整数 ID 编码  | 差（易引入虚假顺序） | 低      |
| [[One-Hot编码]]      | 用二进制向量编码   | 差（维度爆炸）    | 低      |


### 七、总结
Target Encoding 是处理类别特征的高效工具，尤其适合高基数场景。其核心是利用目标变量的统计信息，但需警惕数据泄露和过拟合风险。实际应用中，建议结合交叉验证编码、平滑处理和噪声添加，以平衡性能与稳健性。对于低基数或有序类别特征，Label Encoding 可能更简单有效；而高基数、无序特征则优先考虑 Target Encoding。