## 一、LDA的核心目标  
LDA的本质是**寻找最优投影方向**：假设高维样本$x \in \mathbb{R}^d$，通过投影向量$w \in \mathbb{R}^d$将其映射到低维空间$y = w^T x$（$y \in \mathbb{R}^k$，$k \ll d$），使得：  
- **类内紧凑性**：同一类样本的投影点尽可能集中（类内散度最小）；  
- **类间分离性**：不同类样本的投影点尽可能远离（类间散度最大）。  


## 二、数学基础：散度矩阵的定义  
为量化“类内紧凑”和“类间分离”，需定义两个关键矩阵：  


### 1. 类内散度矩阵（Within-Class Scatter Matrix）  
衡量**同一类样本内部的分散程度**，定义为所有类别的样本相对于自身类中心的散度之和：  
$$S_w = \sum_{c=1}^C S_c$$  
其中$C$为类别数，$S_c$为第$c$类的散度矩阵：  
$$S_c = \sum_{x \in C_c} (x - \mu_c)(x - \mu_c)^T$$  
$\mu_c = \frac{1}{N_c} \sum_{x \in C_c} x$为第$c$类的样本均值，$N_c$为第$c$类的样本数。  


### 2. 类间散度矩阵（Between-Class Scatter Matrix）  
衡量**不同类别中心之间的分散程度**，定义为各类中心相对于全局中心的散度加权和：  
$$S_b = \sum_{c=1}^C N_c (\mu_c - \mu)(\mu_c - \mu)^T$$  
其中$\mu = \frac{1}{N} \sum_{c=1}^C \sum_{x \in C_c} x = \frac{1}{N} \sum_{c=1}^C N_c \mu_c$为全局样本均值，$N = \sum_{c=1}^C N_c$为总样本数。  


## 三、二分类LDA的推导（核心步骤）  
以二分类（$C=2$）为例，推导最优投影方向$w$：  


### 1. 投影后的统计量  
样本$x$投影到$w$方向后为$y = w^T x$，则：  
- 第1类投影后的均值：$\bar{y}_1 = w^T \mu_1$；  
- 第2类投影后的均值：$\bar{y}_2 = w^T \mu_2$；  
- 投影后的类内散度（方差）：$s_1^2 = w^T S_1 w$，$s_2^2 = w^T S_2 w$，总类内散度$s_w^2 = s_1^2 + s_2^2 = w^T (S_1 + S_2) w = w^T S_w w$；  
- 投影后的类间距离：$(\bar{y}_1 - \bar{y}_2)^2 = (w^T (\mu_1 - \mu_2))^2 = w^T (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T w = w^T S_b w$（二分类时$S_b = N_1 N_2 / N \cdot (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T$，核心项与$(\mu_1 - \mu_2)(\mu_1 - \mu_2)^T$成比例）。  


### 2. 目标函数：最大化类间/类内散度比  
LDA的优化目标是让投影后“类间距离大、类内紧凑”，因此定义目标函数$J(w)$为：  
$$J(w) = \frac{(\bar{y}_1 - \bar{y}_2)^2}{s_1^2 + s_2^2} = \frac{w^T S_b w}{w^T S_w w}$$  


### 3. 求解最优投影方向$w$  
目标是最大化$J(w)$，这是一个**瑞利商（Rayleigh Quotient）** 问题。对$J(w)$关于$w$求导并令导数为0，可得：  
$$S_b w = \lambda S_w w$$  
即$w$是$S_b$关于$S_w$的**广义特征向量**。由于二分类时$S_b w$的方向与$(\mu_1 - \mu_2)$一致（$S_b$的秩为1），最优解可简化为：  
$$w = S_w^{-1} (\mu_1 - \mu_2)$$  


## 四、多类LDA的扩展  
当类别数$C > 2$时，LDA的目标是将数据投影到**$k$维空间**（通常$k \leq C-1$，因$S_b$的秩最多为$C-1$），步骤如下：  


### 1. 目标函数  
多类LDA的目标函数仍为类间散度与类内散度的比值，但扩展为矩阵形式：  
$$J(W) = \frac{\text{tr}(W^T S_b W)}{\text{tr}(W^T S_w W)}$$  
其中$W = [w_1, w_2, ..., w_k] \in \mathbb{R}^{d \times k}$为投影矩阵，$\text{tr}(\cdot)$为矩阵的迹（对角线元素之和）。  


### 2. 求解投影矩阵$W$  
最大化$J(W)$等价于求解**广义特征值问题**：  
$$S_b W = \lambda S_w W$$  
选取$S_w^{-1} S_b$的**前$k$个最大特征值对应的特征向量**组成投影矩阵$W$，通过$y = W^T x$完成高维到$k$维的投影。  


## 五、LDA与PCA的对比  
LDA和PCA均为降维方法，但核心差异显著：  

| 维度         | 线性判别分析（LDA）               | 主成分分析（PCA）                 |  
|--------------|-----------------------------------|-----------------------------------|  
| **监督性**   | 监督学习（利用类别标签）          | 无监督学习（不利用类别标签）      |  
| **目标**     | 最大化类间可分性（提升分类效果）  | 最大化数据方差（保留信息）        |  
| **降维维度** | 最多$C-1$维（$C$为类别数）        | 最多$d$维（原特征数）             |  
| **应用场景** | 分类任务的特征预处理              | 数据可视化、去噪、压缩等          |  


## 六、LDA的优缺点与改进  
### 优点  
- 降维的同时保留分类信息，提升后续分类器效率；  
- 计算简单，对高维数据处理高效；  
- 对噪声数据有一定稳健性。  


### 缺点  
- 假设数据服从**高斯分布且各类协方差相同**，若违反假设，性能会下降；  
- 当样本数远小于特征数时，$S_w$可能奇异（不可逆），需通过正则化（如加入$\lambda I$）解决；  
- 仅能捕捉线性可分关系，对非线性数据效果差（可通过核方法扩展为Kernel LDA）。  


## 七、应用场景  
- **人脸识别**：将高维人脸图像投影到低维空间，保留类间差异（不同人），压缩类内差异（同一人不同角度）；  
- **文本分类**：将高维词向量投影到低维，增强不同类别文本的区分度；  
- **疾病诊断**：对基因数据降维，保留与疾病相关的特征差异。  


## 总结  
LDA是一种兼顾降维和分类的监督学习方法，通过优化类间/类内散度比找到最优投影方向，核心优势是在降低数据维度的同时最大化类别可分性。其思想简单直观，广泛应用于模式识别、计算机视觉等领域，是理解监督降维的基础工具。
