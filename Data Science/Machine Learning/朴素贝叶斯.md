
朴素贝叶斯分类（Naive Bayes Classification）是基于贝叶斯定理和特征条件独立假设的分类算法，因其简单高效而广泛应用于文本分类、垃圾邮件过滤等场景。以下是其核心原理、数学基础及应用特点：

### 1. 贝叶斯定理与朴素假设
- **贝叶斯定理**：  
  对于类别 $C$ 和特征向量 $X=(x_1,x_2,\dots,x_n)$，后验概率为  
  $$P(C|X)=\frac{P(X|C)\cdot P(C)}{P(X)}$$  
  其中  
  - $P(C|X)$：给定特征 $X$ 下类别 $C$ 的后验概率  
  - $P(X|C)$：类别 $C$ 下特征 $X$ 的似然  
  - $P(C)$：类别 $C$ 的先验概率  
  - $P(X)$：特征 $X$ 的边缘概率（归一化常数）

- **朴素假设**：  
  假设特征**条件独立**，即  
  $$P(X|C)=\prod_{i=1}^{n}P(x_i|C)$$  
  > **修正说明**：现实中特征往往相关，但该假设仍能在多数任务中取得良好效果。

---

### 2. 分类决策规则
选择使后验概率最大的类别  
$$\hat{C}=\arg\max_{C}P(C|X)=\arg\max_{C}P(C)\prod_{i=1}^{n}P(x_i|C)$$  
（分母 $P(X)$ 对所有类别相同，可省略）

---

### 3. 常见变体
| 变体 | 适用特征 | 条件概率模型 |
|---|---|---|
| **高斯朴素贝叶斯** | 连续值 | $$P(x_i|C)=\frac{1}{\sqrt{2\pi\sigma_{C}^{2}}}\exp\left(-\frac{(x_i-\mu_C)^2}{2\sigma_C^2}\right)$$ |
| **多项式朴素贝叶斯** | 离散计数 | $$P(x_i|C)=\frac{N_{ci}+\alpha}{N_c+\alpha n}$$ |
| **伯努利朴素贝叶斯** | 二元特征 | $$P(x_i|C)=P(x_i=1|C)^{x_i}\bigl(1-P(x_i=1|C)\bigr)^{1-x_i}$$ |

> **修正**：$\alpha$ 为拉普拉斯平滑系数，避免零概率；多项式模型不仅限词频，也适合其他计数特征。

---

### 4. 训练与预测流程
1. **训练**  
   - 计算先验 $$P(C)=\frac{\text{类别}C\text{样本数}}{\text{总样本数}}$$  
   - 计算条件概率（按上表公式）
2. **预测**  
   - 计算 $$\text{score}(C)=P(C)\prod_{i=1}^{n}P(x_i|C)$$  
   - 返回 $\arg\max_C \text{score}(C)$

---

### 5. 优缺点
| **优点** | **缺点** |
|---|---|
| 训练、预测均 $O(n)$ 复杂度 | 条件独立假设可能不成立 |
| 对缺失值鲁棒 | 对连续变量需假设分布 |
| 易于在线更新 | 无法捕捉特征交互 |

---

### 6. 应用场景
- 文本分类：垃圾邮件、情感分析  
- 推荐系统：用户-物品二分类  
- 医疗诊断：症状-疾病映射  
- 实时流处理：Storm、Flink 中的轻量级分类器

---

### 7. 代码示例（scikit-learn）
```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
corpus = ["good movie", "bad movie", "awesome film", "terrible film"]
y = [1, 0, 1, 0]
vec = CountVectorizer()
X = vec.fit_transform(corpus)
clf = MultinomialNB().fit(X, y)
print(clf.predict(vec.transform(["great film"])))   # [1]
```

---

### 总结
朴素贝叶斯在文本与离散特征场景下表现优异，尽管假设较强，但通过合适的变体（高斯/多项式/伯努利）与平滑技巧，仍能在实际业务中提供快速且可解释的基线模型。
