Stacking（堆叠集成）是[[Ensemble Models 集成学习]]中一种更复杂、更灵活的方法，其核心是通过“两层学习”机制整合多个基学习器的预测结果：第一层由多个基学习器（base learner）生成“中间预测结果”，第二层由一个元学习器（meta-learner）基于这些中间结果学习最终预测。相比简单的平均法或投票法，Stacking能捕捉基学习器之间的复杂关联，因此在很多场景下能取得更优的性能。

[[Meta Learning]]
### 一、核心思想
Stacking的本质是**“学习如何学习”**：  
- 首先，训练多个不同类型的基学习器（如[[决策树]]、[[SVM]]、神经网络等），利用它们对数据的“多角度理解”生成多样化的预测结果；  
- 然后，将这些基学习器的预测结果作为“新特征”（称为“元特征”），训练一个元学习器，让其学习“如何组合这些元特征”才能得到最优最终结果。  

这种机制突破了简单组合策略（如平均、投票）的固定规则限制，能自适应地捕捉基学习器预测之间的非线性关系，因此理论上更接近“最优组合”。[[集成学习的结合策略#一、平均法（适用于回归任务）]] & [[集成学习的结合策略#二、投票法（适用于分类任务）]]


### 二、Stacking的完整流程
Stacking的实现流程较为复杂，核心是通过**交叉验证避免数据泄露**（即基学习器的预测结果不能直接使用训练集自身，否则元学习器会过拟合）。具体步骤如下：


#### 步骤1：数据准备  
设原始训练集为$D = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$，其中$x_i$为输入特征，$y_i$为标签（分类任务为类别，回归任务为连续值）；测试集为$T = \{x_{n+1}, x_{n+2}, ..., x_{n+m}\}$。  


#### 步骤2：基学习器训练与元特征生成（关键步骤）  
这一步的目标是为训练集$D$生成“元特征矩阵”（由基学习器的预测结果组成），同时为测试集$T$生成对应的元特征。  

为避免过拟合，基学习器的预测需通过**k折交叉验证**实现，具体过程如下（以单个基学习器$h$为例）：  
1. 将训练集$D$随机分为$k$个互斥的子集$D_1, D_2, ..., D_k$（k折交叉验证）；  
2. 对于第$i$折（$i=1,2,...,k$）：  
   - 用$D \setminus D_i$（即除$D_i$外的所有数据）训练基学习器$h$，得到模型$h_i$；  
   - 用$h_i$对$D_i$中的样本进行预测，得到$D_i$中每个样本的预测值$z_{x \in D_i}$（对于分类任务，可为类别概率；回归任务为连续值）；  
3. 完成k折后，将所有$D_i$的预测值拼接，得到基学习器$h$对整个训练集$D$的元特征$Z_h = [z_1, z_2, ..., z_n]$（$z_j$为第$j$个样本的预测值）；  
4. 用整个训练集$D$重新训练基学习器$h$，得到最终模型$h_{full}$，并用其对测试集$T$预测，得到测试集的元特征$Z_h^test = [z_{n+1}, ..., z_{n+m}]$。  

对所有基学习器重复上述过程，假设共有$m$个基学习器，则训练集的元特征矩阵为$Z = [Z_{h_1}, Z_{h_2}, ..., Z_{h_m}]^T$（每行对应一个样本，每列对应一个基学习器的元特征），测试集的元特征矩阵为$Z^{test} = [Z_{h_1}^{test}, ..., Z_{h_m}^{test}]^T$。  


#### 步骤3：元学习器训练  
元学习器（如线性回归、逻辑回归、随机森林等）以训练集的元特征矩阵$Z$为输入，以原始标签$y = [y_1, y_2, ..., y_n]$为输出进行训练，得到模型$H$。即：  
$$
H = train(Z, y)
$$  


#### 步骤4：最终预测  
用训练好的元学习器$H$，对测试集的元特征矩阵$Z^{test}$进行预测，得到最终结果：  
$$
\hat{y}^{test} = H(Z^{test})
$$  


### 三、关键组件  
Stacking的性能高度依赖两个核心组件的设计：  


#### 1. 基学习器（Base Learners）  
- **作用**：提供多样化的中间预测结果（元特征），是Stacking的“信息源”。  
- **选择原则**：  
  - **多样性**：基学习器应具有不同的学习机制（如树模型+线性模型+神经网络），避免预测结果高度相关（否则元特征信息冗余）；  
  - **稳定性**：单个基学习器性能需“合格”（至少优于随机猜测），太差的基学习器会污染元特征。  
- 常见选择：随机森林、XGBoost、LightGBM、SVM、神经网络等。  


#### 2. 元学习器（Meta-learner）  
- **作用**：学习如何最优组合基学习器的元特征，是Stacking的“决策者”。  
- **选择原则**：  
  - **简单性优先**：元学习器通常选择简单模型（如线性回归、逻辑回归），避免过拟合（复杂模型易记住基学习器的噪声）；  
  - **适配任务**：分类任务用分类模型（如逻辑回归、SoftMax回归），回归任务用回归模型（如线性回归、Ridge回归）。  
- 常见选择：逻辑回归（分类）、线性回归（回归）、随机森林（需控制复杂度）。  


### 四、优势与局限性  


#### 优势  
1. **性能潜力高**：通过元学习器自适应学习组合策略，能捕捉基学习器之间的复杂关系（如互补性），常优于简单组合策略；  
2. **灵活性强**：基学习器和元学习器可自由选择，适配不同数据类型（结构化数据、图像、文本等）；  
3. **通用性好**：对基学习器类型无限制，可整合不同模型的优势（如树模型的非线性捕捉+线性模型的稳定性）。  


#### 局限性  
1. **实现复杂**：需设计基学习器、元学习器，且需严格处理交叉验证避免数据泄露，工程实现难度高于Bagging/Boosting；  
2. **计算成本高**：基学习器的k折交叉验证+元学习器训练，计算量远大于简单集成方法；  
3. **过拟合风险**：若基学习器多样性不足、元学习器过于复杂，或交叉验证设计不合理，易导致过拟合。  


### 五、与其他集成方法的对比  
| 集成方法       | 核心机制                          | 灵活性     | 计算成本 | 过拟合风险 |  
|----------------|-----------------------------------|------------|----------|------------|  
| 平均/投票法    | 固定规则组合基学习器输出          | 低（规则固定） | 低       | 低         |  
| Bagging（如RF）| 并行训练同类型基学习器，简单平均  | 中（同类型模型） | 中       | 低         |  
| Boosting（如GBDT）| 串行训练基学习器，加权组合       | 中（同类型模型） | 中       | 中         |  
| Stacking       | 元学习器学习组合基学习器的元特征  | 高（跨类型模型） | 高       | 中-高      |  


### 六、实际应用注意事项  
1. **基学习器多样性**：优先选择不同类型的模型（如树模型+线性模型+神经网络），避免“同质化”（如多个随机森林）；  
2. **交叉验证设计**：k折交叉验证的k值通常取5或10，确保元特征的可靠性；  
3. **元特征维度控制**：基学习器数量不宜过多（通常5-10个），否则元特征维度高，元学习器易过拟合；  
4. **避免数据泄露**：必须通过交叉验证生成训练集的元特征，禁止用基学习器在整个训练集上的预测直接作为元特征（会导致元学习器“看到”训练数据的标签信息）。  


### 总结  
Stacking是一种“分层集成”方法，通过基学习器生成元特征、元学习器学习组合策略，实现了比简单集成更精细的预测。尽管其实现复杂且计算成本高，但在需要极致性能的场景（如竞赛、高价值预测任务）中，常能成为“制胜法宝”。核心是平衡基学习器的多样性与元学习器的简单性，同时严格控制数据泄露风险。