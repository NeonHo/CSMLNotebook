### BN 正则化效果与 Dropout 的异同解析

[[Batch Normalization]]（BN）与 [[Dropout]] 虽机制不同，却都通过**引入训练过程的随机性或约束模型对输入细节的过度依赖**，抑制过拟合，提升泛化能力。具体原因可从以下角度展开。

---

### 1. 训练时的随机性：批次统计量的波动

BN 在训练阶段依赖**当前批次的均值和方差**（而非全局统计量）。不同批次样本组成不同，导致：

- 同一输入 $x_i$ 在批次 A 与批次 B 中被归一化为  
  $\hat{x}_{i}^{A} \neq \hat{x}_{i}^{B}$  
  这种“批次噪声”相当于给输入加入扰动，迫使模型学习对波动不敏感的鲁棒特征。

- 与 Dropout 类似：  
  Dropout 随机丢弃神经元 → 引入输出扰动；  
  BN 随机批次统计 → 引入输入扰动。  
  二者均通过破坏确定性防止过拟合。

---

### 2. 削弱神经元对绝对数值的依赖

- **Dropout**：神经元可能被随机置零，网络无法依赖特定神经元，必须学习冗余、通用特征组合。  
- **BN**：通过归一化将输入标准化为均值 0、方差 1，使神经元关注**相对变化**而非绝对阈值；  
  同时，批次内其他样本会间接影响当前样本的统计量，进一步稀释异常值的影响，减少过拟合。

---

### 3. 训练-推理差异的隐式约束

| 阶段 | 统计来源 | 效果 |
|---|---|---|
| **训练** | 批次统计量 $\mu_B,\sigma_B^2$ | 需适应批次波动 |
| **推理** | 全局移动平均 | 需在稳定统计量下工作 |

这种“适应不一致性”迫使模型学习**跨批次稳健特征**，起到类似数据增强的正则化作用。

---

### 4. 机制对比表

| 特性 | Dropout | Batch Normalization |
|---|---|---|
| **随机性来源** | 神经元随机置零 | 批次统计量波动 |
| **作用对象** | 神经元输出 | 输入数据分布 |
| **正则化本质** | 破坏共适应 | 引入输入扰动 |
| **副作用** | 训练时间↑ | 小批次统计不稳 |

---

### 总结

BN 的正则化效果源于 **批次统计噪声** + **对绝对数值依赖的削弱** + **训练-推理差异的隐式约束**。  
虽然效果通常弱于 Dropout，但二者可通过调整位置或参数协同使用。实际部署时需根据任务、批次大小与网络结构灵活选择。