### One-Hot 编码（One-Hot Encoding）

One-Hot 编码是一种将**离散型类别变量**转换为**数值型向量**的常用方法，核心思想是“有多少个类别，就用多少维向量，其中仅对应类别的那一维为 1，其余全为 0”。

#### 1. 基本公式  
若有 $K$ 个类别，类别索引记为 $c \in \{0,1,\dots,K-1\}$，则该类别对应的 One-Hot 向量为  
$$
\mathbf{y} = [0, \dots, 0, \underbrace{1}_{\text{第 }c\text{ 位}}, 0, \dots, 0] \in \mathbb{R}^K
$$
即  
$$
y_i = \begin{cases}
1, & i = c \\[4pt]
0, & i \ne c
\end{cases}
$$

#### 2. 举例  
假设类别集合为 $\{\text{猫}, \text{狗}, \text{鸟}\}$，则  
- “猫” → $[1, 0, 0]$  
- “狗” → $[0, 1, 0]$  
- “鸟” → $[0, 0, 1]$

#### 3. 主要用途  
- **分类标签**：作为 softmax 输出对应的真值向量，用于计算交叉熵损失。  
- **输入特征**：将离散特征（如颜色、城市）转换为神经网络可接受的数值输入。  

#### 4. 优缺点  
| 优点 | 缺点 |
|---|---|
| 简单直观，易于实现 | 维度随类别数线性增长，容易出现高维稀疏 |
| 无大小关系，避免类别间虚假序关系 | 对类别量大的场景（如词汇表）存储与计算开销大 |

#### 5. 常见扩展  
- **稀疏表示**：用 `(index, 1)` 的方式存储，节省内存。  
- **嵌入层替代**：在大类别场景（如 NLP 字词）用低维稠密嵌入（Embedding）代替 One-Hot，缓解维度灾难。