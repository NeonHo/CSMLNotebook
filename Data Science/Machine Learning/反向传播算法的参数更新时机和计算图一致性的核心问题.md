[[Back Propagation]]
### 参数更新时机的数学与工程解释

> 你的疑问：  
> “为什么计算倒数第二层的梯度时，倒数第一层的参数还没更新，却依然有效？”

下面从**数学原理、计算图一致性、工程实践、优化策略**四个角度给出完整回答。

---

### 一、数学原理：梯度基于“当前参数快照”

1. **链式法则要求固定图**  
   反向传播计算损失 $L$ 对参数 $\mathbf{W}$ 的梯度时，每一步都使用**本轮前向传播结束后的参数值**，即一次“快照”：
   $$
   \left.\frac{\partial L}{\partial \mathbf{W}^{(l)}}\right|_{\text{step }t}
   \text{ 与 } \mathbf{W}^{(l)}_t \text{ 固定}
   $$

2. **更新公式**  
   所有梯度计算完毕后，再统一更新：  
   $$
   \mathbf{W}^{(l)}_{t+1} = \mathbf{W}^{(l)}_t - \alpha \left.\frac{\partial L}{\partial \mathbf{W}^{(l)}}\right|_t
   $$  
   因此 **不存在“中途改变”的梯度**，计算与更新是**两阶段分离**的。

---

### 二、计算图一致性：为何不能边算边更新

| 情形 | 后果 |
|---|---|
| 边算梯度边更新参数 | 破坏链式法则前提，梯度不再是真实梯度 |
| 统一更新 | 保证本轮所有梯度均来自同一“前向图”，数学正确 |

---

### 三、实践效果：小步长 + 批统计保证收敛

- **学习率 $\alpha$ 足够小** 时，每步更新幅度极小，下一轮前向图几乎不变。  
- **Mini-Batch** 带来梯度噪声，反而起到正则化作用，抵消“参数不匹配”带来的误差。

---

### 四、改进策略（可选）

| 方法 | 思路 | 备注 |
|---|---|---|
| **梯度累积** | 多批次累加后再更新 | 等价于增大 batch size |
| **二阶优化** | 牛顿/L-BFGS | 计算代价高，大模型少用 |
| **异步 SGD** | 分布式节点独立更新 | 需额外调参，工程折中 |

---

### 五、结论

- **数学正确**：链式法则要求固定计算图。  
- **工程简洁**：统一更新避免复杂依赖。  
- **实践有效**：小步长与批梯度天然容错。  

因此，**“先算完所有梯度再一次性更新”** 是反向传播的**必要设计**，并非缺陷。现代框架（PyTorch、TensorFlow）都严格遵守这一机制。