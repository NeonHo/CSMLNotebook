
# 词向量模型：从词语表示到语义理解的核心技术

## 一、词向量模型的本质与意义

词向量（[[Word Embedding]]）是自然语言处理（NLP）中用于将词语转换为连续稠密向量的技术，解决了传统[[One-Hot编码]]（高维稀疏、无法表示语义关系）的缺陷。其核心思想是：**“上下文相似的词，语义也相似”**，通过数学映射将词语嵌入到低维空间（如50-300维），使向量距离反映语义相关性（如“国王-男人+女人≈王后”）。

## 二、经典词向量模型详解

### 1. [[Word2Vec]]（2013，Mikolov等）
**原理**：基于神经网络预测词语上下文，通过优化语言模型学习词向量，包含两种架构：
- **CBOW（连续词袋模型）**：用上下文词语预测当前词（如用“国王”“女人”预测“王后”）。
- **Skip-gram**：用当前词预测上下文（如用“王后”预测“国王”“女人”）。

**优化技巧**：
- **负采样（Negative Sampling）**：减少训练时的负例计算量。
- **层次化Softmax（Hierarchical Softmax）**：用哈夫曼树降低分类复杂度。

**特点**：训练速度快，词向量能捕捉词的语义相似性（如“苹果”与“香蕉”距离近），但属于**静态词向量**（一词一向量，不考虑上下文）。

### 2. GloVe（2014，Pennington等）
**原理**：结合**全局词共现统计信息**与神经网络，通过矩阵分解学习词向量。公式如下：
$$
w_i^T \tilde{w}_j + b_i + \tilde{b}_j = \log(X_{ij})
$$
其中 \(X_{ij}\) 表示词 \(i\) 和词 \(j\) 在语料库中的共现次数。

**特点**：
- 兼顾Word2Vec的局部上下文和LSA（潜在语义分析）的全局统计，向量质量更高。
- 对低频词表现更好（Word2Vec依赖上下文出现频率）。

### 3. FastText（2016，Facebook）
**原理**：在Word2Vec基础上引入**子词（Subword）单元**，将词语拆分为字符n-gram（如“apple”拆分为“<app”, “appl”, “pple”, “ple>”），每个词的向量是其所有子词向量的均值。

**优势**：
- 有效处理未登录词（OOV，如新词“元宇宙”）。
- 训练速度极快，适合处理海量文本（如社交媒体短文本）。
- 在文本分类任务中表现突出（因考虑了词的内部结构）。

### 4. ELMo（2018，Peters等）
**原理**：基于双向LSTM的**动态词向量模型**，通过预训练语言模型（前向LM和后向LM）生成上下文相关的词向量。

**特点**：
- 同一词语在不同语境中向量不同（如“苹果”在“水果”和“公司”中向量不同）。
- 开创了“预训练+微调”模式，为BERT等模型奠定基础。
- **局限**：LSTM建模能力有限，上下文依赖长度较短。

### 5. BERT（2018，Google）
**原理**：基于**双向Transformer编码器**，通过“掩码语言模型（MLM）”和“下一句预测（NSP）”预训练，生成深度上下文相关的词向量。

**突破**：
- 双向建模（ELMo是浅层双向，BERT是深层双向），更精准捕捉语义依赖（如“他说：‘我很开心’”中“我”指向“他”）。
- 预训练模型可微调至各类任务（如问答、情感分析），推动NLP进入“大模型时代”。

**变种**：RoBERTa（优化预训练策略）、ALBERT（参数压缩）等。

### 6. GPT系列（2018-2023，OpenAI）
- **GPT-1/2/3/4**：基于**单向Transformer解码器**，通过自回归预训练（预测下一个词）生成词向量，本质是动态词向量与语言生成的结合。
- **特点**：
  - 不依赖任务微调，仅通过提示（Prompt）即可完成推理、翻译等任务（如GPT-3的“零样本学习”）。
  - 模型规模极大（GPT-4参数量超万亿），语义理解和生成能力接近人类水平。

## 三、词向量模型对比表格

| 模型         | 核心架构       | 上下文相关性 | 训练目标               | 优势场景                     |
|--------------|----------------|--------------|------------------------|------------------------------|
| Word2Vec     | 神经网络       | 静态         | 预测上下文             | 基础词向量、快速原型         |
| GloVe        | 矩阵分解       | 静态         | 拟合词共现统计         | 学术研究、需全局语义的任务   |
| FastText     | 子词+神经网络  | 静态         | 预测上下文+子词建模    | 低资源语言、OOV处理         |
| ELMo         | 双向LSTM       | 动态         | 双向语言模型           | 需上下文敏感的任务（如歧义） |
| BERT         | 双向Transformer| 动态         | 掩码语言模型+下一句预测| 复杂NLP任务（问答、分类）   |
| GPT          | 单向Transformer| 动态         | 自回归语言生成         | 文本生成、少样本学习         |

## 四、词向量的应用与延伸

### 1. 基础应用
- **语义相似度计算**：如判断“计算机”和“电脑”的向量余弦相似度。
- **词聚类**：将相似语义的词（如“狗”“猫”）聚为一类。
- **文本表示**：将句子/文档表示为词向量的均值、加权和或通过Doc2Vec模型。

### 2. 下游任务
- **分类任务**：情感分析（如“这部电影很精彩”→正面）、垃圾邮件识别。
- **序列标注**：命名实体识别（NER，如识别“北京”为地点）。
- **生成任务**：机器翻译、文本摘要（如GPT系列模型）。

### 3. 进阶模型
- **跨语言词向量**：如XLM-R，实现多语言语义空间对齐（“apple”和“苹果”在中英向量空间中接近）。
- **图神经网络词向量**：如GraphSAGE，结合知识图谱结构学习词向量（如“巴黎”与“法国”的向量关联）。

## 五、总结：从静态到动态的演进逻辑

- **静态词向量（Word2Vec/GloVe）**：适合处理语义稳定的场景（如百科文本），但无法解决一词多义（如“bank”在“银行”和“河岸”中的歧义）。
- **动态词向量（ELMo/BERT/GPT）**：通过上下文建模解决歧义，成为当前主流，但计算成本较高。

选择模型时，需根据任务需求（生成/分类）、数据规模（小数据用FastText，大数据用BERT）和硬件资源（GPT-4需大规模算力）综合考量。
