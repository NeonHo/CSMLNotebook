# 支持向量回归（SVR）解析

支持向量机（SVM）用于回归任务时被称为**支持向量回归（Support Vector Regression, SVR）**。与分类任务不同，SVR的目标是找到一个能拟合数据的连续函数，同时容忍一定范围内的误差。以下从核心原理、关键概念、数学推导及算法流程等方面详细解析：

## 一、SVR与SVM分类的本质区别
[[SVM]]

| **维度**       | **SVM分类**                          | **SVR回归**                          |
|----------------|-------------------------------------|-------------------------------------|
| **目标**       | 寻找超平面最大化类别间隔               | 寻找函数使预测值与真实值偏差不超过ε，同时函数平滑 |
| **损失函数**   | 铰链损失（Hinge Loss）               | ε-不敏感损失（ε-Insensitive Loss）       |
| **核心假设**   | 样本可被超平面分隔                    | 大部分样本落在“ε-管”内，仅少数点影响模型   |
| **支持向量**   | 离超平面最近的样本点                  | 落在ε-管外或管边界上的样本点            |

## 二、SVR的核心概念：ε-不敏感损失与ε-管

### 1. **ε-不敏感损失函数**
- **定义**：若预测值 $f(x)$ 与真实值 $y$ 的偏差不超过阈值 $\varepsilon$，则视为“无损失”；超过 $\varepsilon$ 时，损失与偏差成正比。  
  数学表达式：

  $$
  L_\varepsilon(y, f(x)) = 
  \begin{cases} 
  0, & \text{若 } |y - f(x)| \leq \varepsilon \\
  |y - f(x)| - \varepsilon, & \text{否则}
  \end{cases}
  $$

- **直观理解**：构建一个以预测函数为中心、宽度为 $2\varepsilon$ 的“ε-管”，管内的点不计算损失，仅管外的点产生损失。

### 2. **优化目标：兼顾拟合精度与模型复杂度**
- **目标函数**：

  $$
  \min_{w,b,\xi,\xi^*} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n (\xi_i + \xi_i^*)
  $$

  其中：
  - $\frac{1}{2}\|w\|^2$ 为正则项，控制模型复杂度；
  - $C$ 为惩罚参数，平衡正则项与拟合误差；
  - $\xi_i, \xi_i^*$ 为松弛变量，允许样本超出ε-管（$\xi_i \geq 0$ 表示预测值大于真实值+ε，$\xi_i^* \geq 0$ 表示预测值小于真实值-ε）。

## 三、SVR的数学推导与对偶问题

### 1. **原始优化问题**
给定训练集 $\{(x_i, y_i)\}_{i=1}^n$，SVR的原始问题可表示为：

$$
\begin{align*}
\min_{w,b,\xi,\xi^*} &\ \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n (\xi_i + \xi_i^*) \\
\text{s.t.} &\ y_i - f(x_i) \leq \varepsilon + \xi_i, \\
&\ f(x_i) - y_i \leq \varepsilon + \xi_i^*, \\
&\ \xi_i, \xi_i^* \geq 0,\ i=1,2,\dots,n.
\end{align*}
$$

其中 $f(x) = w^T x + b$ 为线性回归函数。

### 2. **引入拉格朗日乘数转化为对偶问题**
通过拉格朗日乘数法，构造拉格朗日函数：

$$
L = \frac{1}{2}\|w\|^2 + C\sum_{i=1}^n (\xi_i + \xi_i^*) - \sum_{i=1}^n \alpha_i (\varepsilon + \xi_i - y_i + w^T x_i + b) - \sum_{i=1}^n \alpha_i^* (\varepsilon + \xi_i^* + y_i - w^T x_i - b) - \sum_{i=1}^n (\mu_i \xi_i + \mu_i^* \xi_i^*)
$$

其中 $\alpha_i, \alpha_i^*, \mu_i, \mu_i^* \geq 0$ 为拉格朗日乘数。

对 $w, b, \xi_i, \xi_i^*$ 求偏导并令其为0，消去原始变量后得到对偶问题：

$$
\begin{align*}
\max_{\alpha,\alpha^*} &\ \sum_{i=1}^n y_i (\alpha_i^* - \alpha_i) - \varepsilon \sum_{i=1}^n (\alpha_i^* + \alpha_i) - \frac{1}{2} \sum_{i,j=1}^n (\alpha_i^* - \alpha_i)(\alpha_j^* - \alpha_j) x_i^T x_j \\
\text{s.t.} &\ \sum_{i=1}^n (\alpha_i^* - \alpha_i) = 0, \\
&\ 0 \leq \alpha_i, \alpha_i^* \leq C,\ i=1,2,\dots,n.
\end{align*}
$$

### 3. **核函数的引入（处理非线性回归）**
与SVM分类类似，通过核函数 $\kappa(x_i, x_j) = \phi(x_i)^T \phi(x_j)$ 将数据映射到高维空间，此时对偶问题中的内积 $x_i^T x_j$ 替换为 $\kappa(x_i, x_j)$，回归函数变为：

$$
f(x) = \sum_{i=1}^n (\alpha_i^* - \alpha_i) \kappa(x_i, x) + b
$$

常用核函数包括：
- **线性核**：$\kappa(x_i, x_j) = x_i^T x_j$
- **径向基函数（RBF）核**：$\kappa(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$，$\gamma > 0$
- **多项式核**：$\kappa(x_i, x_j) = (x_i^T x_j + r)^d$，$r \geq 0, d \geq 1$

## 四、SVR算法流程

1. **数据预处理**：
   - 标准化或归一化特征（避免不同量纲影响模型）；
   - 划分训练集与测试集。
2. **选择核函数与参数**：
   - 核函数：根据数据分布选择（如非线性数据选RBF核）；
   - 调优参数：$C$（控制误差容忍度）、$\varepsilon$（管宽度）、核函数参数（如RBF的$\gamma$）。
3. **构建并训练SVR模型**：
   - 求解对偶问题，得到拉格朗日乘数 $\alpha_i, \alpha_i^*$；
   - 确定支持向量（满足 $0 < \alpha_i < C$ 或 $0 < \alpha_i^* < C$ 的样本点）。
4. **预测与评估**：
   - 用训练好的模型对新样本计算 $f(x)$；
   - 评估指标：均方误差（MSE）、平均绝对误差（MAE）、决定系数（$R^2$）等。

## 五、关键参数的影响

| **参数** | **作用**                                                                 | **调优方向**                                                                 |
|----------|--------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| $C$      | 惩罚参数，控制对ε-管外误差的重视程度：$C$ 越大，模型越不允许误差，易过拟合；$C$ 越小，模型越容忍误差，易欠拟合。 | 小数据集优先尝试中等值（如 $C=1$），再通过交叉验证调整。                          |
| $\varepsilon$ | 误差容忍阈值：$\varepsilon$ 越大，管越宽，包含的样本越多，模型越简单，但拟合精度可能下降。 | 根据数据噪声水平设置：噪声大时增大$\varepsilon$，反之减小。                        |
| $\gamma$（RBF核） | 控制核函数的作用范围：$\gamma$ 越大，高维空间中样本的距离越敏感，模型越复杂；$\gamma$ 越小，模型越平滑。 | 与$C$ 联合调优，常用网格搜索（如 $\gamma \in \{10^{-3}, 10^{-2}, \dots, 10^3\}$）。    |

## 六、SVR的优缺点与应用场景

### 1. **优点**
- **小样本学习**：依赖少量支持向量，适合样本量有限的场景；
- **高维适应性**：通过核函数处理高维数据，无需特征降维；
- **抗噪声能力**：ε-不敏感损失对 outliers 不敏感。

### 2. **缺点**
- **计算复杂度高**：求解二次规划问题的时间复杂度随样本量增加呈立方级增长；
- **参数调优困难**：需同时优化 $C$、$\varepsilon$ 和核函数参数，计算成本高；
- **对大规模数据不友好**：内存需求大，不适用于百万级以上样本。

### 3. **应用场景**
- **回归预测**：房价预测、股票价格趋势、传感器数据平滑；
- **时间序列分析**：流量预测、天气数据建模；
- **工程与科学**：物理实验数据拟合、化学性质预测。

## 七、总结

SVR通过**ε-不敏感损失函数**和**核函数技术**，将SVM的分类思想拓展到回归任务中，核心是在“模型复杂度”与“拟合误差”之间寻找平衡。其本质是构建一个允许一定误差的“预测管”，仅用管外的支持向量确定回归函数。实际应用中，合理选择核函数与调优参数是提升SVR性能的关键，而对于大规模数据，可考虑近似算法（如SVR-GPU）或替代模型（如随机森林回归）。