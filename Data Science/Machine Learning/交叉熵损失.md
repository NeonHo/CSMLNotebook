在机器学习和深度学习中，**交叉熵损失（Cross-Entropy Loss）** 是一种广泛用于分类任务的损失函数，核心作用是衡量模型预测的概率分布与真实标签的概率分布之间的差异。它在二分类、多分类任务中均有重要应用，且数学原理清晰，对模型优化具有良好的指导意义。

### 一、核心原理：概率分布的“距离”度量

交叉熵的概念源于信息论，原本用于衡量两个概率分布之间的“不匹配程度”。在分类问题中：

- **真实分布**：通常用独热编码（one-hot encoding）[[One-Hot编码]]表示，即对于某个样本，只有其真实类别对应的位置为 1，其余为 0（例如，三分类中真实类别为“第二类”，则真实分布为 `[0, 1, 0]`）。
- **预测分布**：模型输出的概率分布（例如，经 softmax 激活后，三分类模型可能输出 `[0.2, 0.7, 0.1]`，表示属于第一、二、三类的概率分别为 20%、70%、10%）。

交叉熵损失的本质是：**让模型预测的概率分布尽可能接近真实分布**。差异越小，损失值越低，模型性能越好。

---

### 二、数学形式与分类场景

#### 1. 多分类任务（Categorical Cross-Entropy）

假设有 $K$ 个类别，样本的真实标签为独热向量 $y = (y_1, y_2, \dots, y_K)$（其中 $y_k = 1$ 表示样本属于第 $k$ 类，其余为 0），模型预测的概率分布为 $\hat{p} = (\hat{p}_1, \hat{p}_2, \dots, \hat{p}_K)$（满足 $\sum_{k=1}^K \hat{p}_k = 1$）。

单个样本的交叉熵损失公式为：  
$$
\text{Loss} = -\sum_{k=1}^K y_k \cdot \log(\hat{p}_k)
$$

**简化理解**：由于真实标签是独热向量，上式中只有真实类别对应的 $y_k = 1$，其余项均为 0，因此损失可简化为 **$- \log(\hat{p}_{\text{true}})$**，其中 $\hat{p}_{\text{true}}$ 是模型对真实类别的预测概率。

- 若模型对真实类别预测的概率越高（如 $\hat{p}_{\text{true}} = 0.9$），损失越小（$- \log(0.9) \approx 0.105$）；  
- 若预测概率越低（如 $\hat{p}_{\text{true}} = 0.1$），损失越大（$- \log(0.1) \approx 2.303$）；  
- 若完全预测错误（$\hat{p}_{\text{true}} \to 0$），损失趋近于无穷大，惩罚极强。

#### 2. 二分类任务（Binary Cross-Entropy）

当类别数 $K=2$ 时，交叉熵损失可简化为“二元交叉熵”，此时：

- 真实标签 $y \in \{0, 1\}$（0 表示负类，1 表示正类）；  
- 模型预测正类的概率为 $\hat{p}$，则负类概率为 $1 - \hat{p}$。

损失公式为：  
$$
\text{Loss} = -[y \cdot \log(\hat{p}) + (1 - y) \cdot \log(1 - \hat{p})]
$$

这一公式与“对数损失（[[LogLoss]]）”完全等价，因此二元交叉熵也常被称为对数损失。

---

### 三、与其他损失函数的对比

| **损失函数**       | **适用场景**       | **特点**                                  |
|--------------------|--------------------|-------------------------------------------|
| 交叉熵损失         | 分类任务（二/多类） | 关注概率分布的匹配，对置信度敏感          |
| 均方误差（MSE）    | 回归任务           | 衡量预测值与真实值的平方差，不适合分类    |
| hinge 损失         | 支持向量机（SVM）  | 仅关注分类是否正确，对置信度不敏感        |

**为何分类任务更适合交叉熵损失？**

- 均方误差在分类任务中可能导致梯度消失（例如，当使用 sigmoid/softmax 激活时，输出接近 0 或 1 时梯度极小）；  
- 交叉熵损失的梯度与预测误差直接相关（$\hat{p} - y$），优化更高效，能更快推动模型收敛。

---

### 四、实际应用场景

1. **图像分类**：如 ResNet、VGG 等模型在多分类任务中，最后一层通过 softmax 输出概率分布，配合交叉熵损失训练。  
2. **自然语言处理**：如文本分类（情感分析、垃圾邮件识别），使用 [[BERT]] 等模型时，常用交叉熵损失优化。  
3. **目标检测**：在分类子任务中（如判断候选框内物体类别），交叉熵损失是核心损失函数之一。

---

### 五、总结

交叉熵损失是分类任务中最常用的损失函数，其核心价值在于：

- 从概率分布的角度衡量模型预测的“合理性”，不仅关注分类是否正确，还关注预测的置信度；  
- 数学性质优良，梯度计算简单，能有效指导模型参数优化；  
- 可灵活扩展到二分类、多分类等场景，适用性极强。

理解交叉熵损失的原理，有助于更深入地掌握分类模型的训练逻辑和性能优化方向。