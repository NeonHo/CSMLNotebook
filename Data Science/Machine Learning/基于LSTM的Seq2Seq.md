基于 LSTM 的 Seq2Seq（Sequence-to-Sequence）模型是一种经典的序列转换模型，广泛应用于机器翻译、文本摘要、对话生成等任务。其核心思想是通过**编码器（Encoder）** 将输入序列（如源语言句子）编码为一个固定长度的“上下文向量”，再通过**解码器（Decoder）** 将该向量解码为输出序列（如目标语言句子）。

---

### **一、Seq2Seq 的整体结构**

Seq2Seq 模型由两部分组成，均基于 [[LSTM]]（Long Short-Term Memory）单元：

- **编码器（Encoder）**：接收输入序列（长度可变），输出一个固定长度的“上下文向量”（Context Vector），蕴含输入序列的语义信息。
- **解码器（Decoder）**：以上下文向量为初始状态，逐步生成输出序列（长度可变），每个时间步生成一个元素（如单词）。

二者通过“上下文向量”连接，实现“可变长输入 → 固定长向量 → 可变长输出”的转换。

---

### **二、训练过程（Training）**

训练的目标是让模型学习从输入序列到输出序列的映射，通过优化损失函数（如交叉熵）更新编码器和解码器的参数。

#### 1. 数据预处理
[[数据预处理]]
- 输入序列（源序列）：如“我爱你”，预处理为分词后的列表 `[“我”, “爱”, “你”]`，并通常添加**结束符号**（如 `<eos>`），表示序列结束：`[“我”, “爱”, “你”, “<eos>”]`。
- 输出序列（目标序列）：如“I love you”，预处理为 `[“<sos>”, “I”, “love”, “you”, “<eos>”]`，其中 `<sos>` 是**开始符号**，表示生成的起点。
- 注意：输入和输出序列需转换为词索引（通过词汇表映射），便于模型处理。

#### 2. 编码器（Encoder）的工作过程

编码器的作用是将输入序列编码为上下文向量，步骤如下：

- **初始化**：编码器 LSTM 的初始隐藏状态 $h_0$ 和细胞状态 $c_0$ 通常设为全 0 向量（形状为 `(隐藏层维度,)`）。
- **逐时间步处理输入序列**：  
  对于输入序列的第 $t$ 个词 $x_t$（已转换为词向量），LSTM 根据当前输入和上一时间步的状态，更新隐藏状态和细胞状态：  
  $$
  h_t, c_t = \text{LSTM}(x_t, (h_{t-1}, c_{t-1}))
  $$  
  其中，$h_t$ 是第 $t$ 步的隐藏状态，$c_t$ 是细胞状态（LSTM 特有的记忆单元，用于缓解梯度消失）。
- **生成上下文向量**：  
  当输入序列处理完毕（即到达 `<eos>`），编码器的**最后一个时间步的隐藏状态 $h_{\text{enc}}$ 和细胞状态 $c_{\text{enc}}$** 被作为上下文向量，传递给解码器。  
  （注：部分模型会拼接所有时间步的隐藏状态作为上下文向量，但基于 LSTM 的基础 Seq2Seq 通常仅用最后一个状态。）

#### 3. 解码器（Decoder）的训练过程（Teacher Forcing）

解码器的目标是学习从上下文向量生成目标序列，训练时采用**Teacher Forcing**策略（用真实序列引导生成），步骤如下：

- **初始化**：解码器 LSTM 的初始隐藏状态 $h_0'$ 和细胞状态 $c_0'$ 设为编码器输出的上下文向量（即 $h_0' = h_{\text{enc}},\ c_0' = c_{\text{enc}}$）。
- **逐时间步生成输出**：  
  目标序列为 `[“<sos>”, y_1, y_2, ..., y_n, “<eos>”]`（共 $T$ 个时间步），解码器在第 $t$ 步的输入是**目标序列的第 $t-1$ 个词 $y_{t-1}$**（而非解码器上一步生成的词）：
  - 第 1 步输入：`<sos>`（生成序列的起点），计算隐藏状态 $h_1'$ 和细胞状态 $c_1'$，输出对 $y_1$ 的概率分布 $P(y_1 | \text{<sos>}, h_{\text{enc}})$。
  - 第 2 步输入：$y_1$，计算 $h_2'$、$c_2'$，输出 $P(y_2 | y_1, h_1')$。
  - …  
  - 第 $T$ 步输入：$y_{T-1}$，输出 $P(\text{<eos>} | y_{T-1}, h_{T-1}')$。
- **计算损失**：  
  每个时间步的输出概率分布与目标词（如 $y_t$）的 One-Hot 向量计算**交叉熵损失**，总损失为所有时间步损失的平均值：
  $$
  \text{Loss} = -\frac{1}{T} \sum_{t=1}^T \log P(y_t | \text{context},\ y_1,\ ...,\ y_{t-1})
  $$  
- **反向传播更新参数**：通过梯度下降（如 Adam）优化编码器和解码器的所有参数（LSTM 的权重、偏置等）。

#### 训练过程的关键特点

- **Teacher Forcing 的优势**：避免解码器早期生成错误词后“累积误差”，加速训练收敛。  
- **潜在问题**：训练时解码器依赖真实序列，而推理时依赖自身生成的词，可能导致“训练-推理不匹配”（Exposure Bias）。

---

### **三、推理过程（Inference）**

推理是模型在训练完成后，根据输入序列生成输出序列的过程（无真实目标序列引导），步骤如下：

#### 1. 编码器处理输入序列

与训练时完全一致：输入序列经分词、转索引后，由编码器 LSTM 逐时间步处理，输出上下文向量 $(h_{\text{enc}}, c_{\text{enc}})$。

#### 2. 解码器生成输出序列

解码器需自主生成序列，从 `<sos>` 开始，直到生成 `<eos>` 或达到最大长度，步骤如下：

- **初始化**：解码器初始状态为 $(h_0' = h_{\text{enc}},\ c_0' = c_{\text{enc}})$，第一个输入为 `<sos>`。
- **逐时间步生成**：
  - 第 1 步：输入 `<sos>`，解码器 LSTM 输出隐藏状态 $h_1'$ 和细胞状态 $c_1'$，再通过线性层 + Softmax 得到词汇表上的概率分布 $P_1$。
  - 选择第 1 个生成词 $\hat{y}_1$：根据概率分布采样（如贪婪搜索：选概率最高的词；或束搜索：保留 Top-K 候选）。
  - 第 2 步：输入 $\hat{y}_1$，重复上述过程，得到 $\hat{y}_2$。
  - …  
  - 终止条件：生成 `<eos>` 或达到预设最大长度（如 50 词），停止生成。

#### 推理的关键策略

- **贪婪搜索（Greedy Search）**：每个时间步选择概率最高的词，速度快但可能陷入局部最优（如重复生成）。  
- **束搜索（Beam Search）**：每个时间步保留 Top-K 个候选序列（束宽 K），最终选得分最高的序列，平衡速度和质量（常用 K=5~10）。  
- **采样（Sampling）**：按概率分布随机选择词，结合 temperature 参数控制多样性（见前文“temperature 因子”）。

---

### **四、训练与推理的核心区别**

| 阶段       | 解码器输入来源          | 目标                          | 关键策略               |
|------------|-------------------------|-------------------------------|------------------------|
| 训练（Training） | 目标序列的真实前序词    | 最小化与真实序列的损失        | Teacher Forcing        |
| 推理（Inference） | 解码器自身生成的前序词  | 生成连贯、符合输入语义的序列  | 贪婪搜索/束搜索/采样   |

---

### **总结**

基于 LSTM 的 Seq2Seq 通过“编码器-解码器”架构实现序列转换：

- **训练**：依赖 Teacher Forcing，用真实目标序列引导解码器学习，通过交叉熵损失优化参数；
- **推理**：解码器自主生成序列，从 `<sos>` 开始，基于自身输出迭代生成，直至 `<eos>`。

该模型的核心是上下文向量的传递，但基础版本存在“长序列信息丢失”“训练-推理不匹配”等问题，后续被 Transformer（如 [[BERT]]、GPT）等模型改进，但 Seq2Seq 的框架仍是序列转换任务的基础思想。