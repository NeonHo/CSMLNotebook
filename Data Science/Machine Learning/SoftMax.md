SoftMax（通常写作 Softmax）是深度学习和机器学习中用于 **多分类任务** 的核心函数，它能将模型输出的“原始得分（Logits）”转化为 **符合概率分布的数值**，使输出结果更易解释（例如，属于每个类别的概率）。

### Softmax 的定义与计算逻辑

假设模型对一个样本的输出是一组“原始得分” $z = [z_1, z_2, ..., z_K]$（其中 $K$ 是类别总数，$z_i$ 表示样本属于第 $i$ 类的原始得分，可正可负），Softmax 函数会将这组得分转化为概率分布 $\hat{y} = [\hat{y}_1, \hat{y}_2, ..., \hat{y}_K]$，公式为：

$$
\hat{y}_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

其中：
- 分子 $e^{z_i}$ 确保每个类别的输出为正数（指数函数特性）；
- 分母是所有类别指数得分的总和，用于“归一化”，使所有 $\hat{y}_i$ 之和为 1，满足概率分布的基本要求（$0 \leq \hat{y}_i \leq 1$ 且 $\sum_{i=1}^{K} \hat{y}_i = 1$）。

---

### 核心作用：将“得分”转化为“可解释的概率”

1. **放大差异，突出高得分类别**  
   指数函数的特性会让“原始得分较高的类别”在概率中占据更大比例。例如：
   - 若原始得分 $z = [3, 1, -2]$，则 Softmax 输出为 $[0.88, 0.12, 0.00]$（近似值），高得分的第 1 类概率显著高于其他类；
   - 若原始得分差异较小（如 $z = [1, 0.9, 0.8]$），则概率分布更平缓（如 $[0.36, 0.33, 0.31]$）。

   这种“放大差异”的特性，能让模型更明确地倾向于高得分类别，便于后续分类决策（例如，取概率最大的类别作为预测结果）。

2. **适配交叉熵损失，实现概率级别的优化**  
   在多分类任务中，Softmax 通常与 **[[交叉熵损失]]（Cross-Entropy Loss）** 结合使用：
   - Softmax 将得分转化为概率；
   - 交叉熵损失通过衡量“预测概率分布”与“真实标签分布（如 one-hot 向量）”的差异，引导模型优化。

   二者结合的损失函数公式为（假设真实标签中第 $c$ 类为 1，其余为 0）：
   $$
   \text{Loss} = -\log\left( \frac{e^{z_c}}{\sum_{j=1}^{K} e^{z_j}} \right)
   $$
   这个损失会迫使模型提高“正确类别”的原始得分 $z_c$，同时降低其他类别的得分，最终让正确类别的概率趋近于 1。

---

### Softmax 的局限性与替代方案

1. **对异常值敏感**  
   若某个类别原始得分 $z_i$ 极大（如远大于其他类），指数函数会导致该类概率接近 1，其他类概率接近 0，可能掩盖模型对其他类的真实判断能力。

2. **计算稳定性问题**  
   当原始得分 $z_i$ 过大时，$e^{z_i}$ 可能因数值溢出导致计算错误。实际应用中，通常会对原始得分做“平移处理”（减去最大值 $\max(z)$），公式变为：
   $$
   \hat{y}_i = \frac{e^{z_i - \max(z)}}{\sum_{j=1}^{K} e^{z_j - \max(z)}}
   $$
   这一处理不改变最终结果（分子分母同比例缩放），但能避免数值溢出。

3. **替代方案**  
   - 对于二分类任务，可用 **Sigmoid 函数**（本质是 Softmax 在 $K=2$ 时的特例）；
   - 若需缓解“过度聚焦单一高得分类别”的问题，可使用 **Log-Softmax**（直接输出对数概率，减少计算量，常见于 PyTorch 等框架）。

---

### 总结

Softmax 是多分类任务中连接“原始得分”与“概率分布”的关键工具，它通过指数归一化将得分转化为可解释的概率，与交叉熵损失配合实现模型优化。尽管存在对异常值敏感等局限，但因其直观性和数学适配性，仍是目前多分类任务的主流选择。