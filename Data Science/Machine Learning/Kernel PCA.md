### 详解Kernel PCA（核主成分分析）


Kernel PCA（Kernel Principal Component Analysis，核主成分分析）是[[PCA]]的扩展，通过**核技巧（Kernel Trick）** 处理非线性数据，实现非线性降维。普通PCA仅能捕捉数据的线性结构，而Kernel PCA能将非线性分布的数据映射到高维空间，再通过PCA进行线性降维，从而在低维空间中保留原数据的非线性特征。


### 一、Kernel PCA的核心思想
普通PCA的核心是寻找高维数据的线性主成分（即方差最大的方向），但对于非线性结构（如螺旋形、环形数据），线性变换无法有效降维。Kernel PCA的解决思路是：  
1. 将低维非线性数据通过**非线性映射**到高维特征空间；  
2. 在高维空间中执行普通PCA，找到线性主成分；  
3. 将高维主成分映射回低维空间，得到非线性降维结果。  

关键在于，通过**核函数**可避免显式的高维映射（避免“维度灾难”），直接计算高维空间中的内积，大幅简化计算。


### 二、核技巧（Kernel Trick）：核心理论基础
核技巧是Kernel PCA的灵魂，其核心是用“核函数”替代高维空间的内积计算，无需显式定义映射函数。

#### 1. 映射函数与内积
设低维空间数据为$x \in \mathbb{R}^d$，通过非线性映射$\phi: \mathbb{R}^d \to \mathbb{R}^k$（$k \gg d$）映射到高维特征空间$\mathbb{R}^k$。高维空间中两点$\phi(x_i)$与$\phi(x_j)$的内积为$\langle \phi(x_i), \phi(x_j) \rangle$。

#### 2. 核函数的定义
核函数$k(x_i, x_j)$是高维内积的低维替代，即：  
$$k(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle$$  

核函数的优势在于：无需知道$\phi$的具体形式，直接在低维空间计算高维内积，避免了高维空间的计算复杂度。


#### 3. 常见核函数
不同核函数适用于不同类型的非线性结构，常见类型如下：

| 核函数类型       | 公式（$x, x'$为样本，$\sigma, c, d$为参数） | 适用场景                     |
|------------------|------------------------------------------|------------------------------|
| 线性核（Linear） | $k(x, x') = x \cdot x'$                  | 线性可分数据（等价于普通PCA） |
| 多项式核（Polynomial） | $k(x, x') = (x \cdot x' + c)^d$          | 多项式非线性关系（如二次曲面） |
| 径向基核（RBF/Gaussian） | $k(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right)$ | 复杂非线性结构（如流形、聚类） |
| Sigmoid核        | $k(x, x') = \tanh(\alpha x \cdot x' + c)$ | 类似神经网络的非线性映射      |


### 三、Kernel PCA的算法步骤
Kernel PCA的核心是通过核矩阵替代高维数据的协方差矩阵，再进行特征分解，具体步骤如下：


#### 步骤1：数据预处理（可选）
对原始数据$X = \{x_1, x_2, ..., x_n\} \in \mathbb{R}^{n \times d}$进行标准化（如均值为0、方差为1），减少量纲对核函数的影响。


#### 步骤2：计算核矩阵$K$
核矩阵$K$是$n \times n$的对称矩阵，其中元素$K_{ij} = k(x_i, x_j)$，表示样本$x_i$与$x_j$在高维空间中的内积。


#### 步骤3：核矩阵的中心化
普通PCA中数据需中心化（均值为0），Kernel PCA中需对高维空间的$\phi(x_i)$中心化。由于$\phi(x_i)$的均值未知，需通过核矩阵间接中心化：  
- 定义中心化矩阵$H = I - \frac{1}{n}11^T$（$I$为单位矩阵，$1$为全1向量）；  
- 中心化核矩阵为：  
  $$\tilde{K} = H K H$$  


#### 步骤4：求解$\tilde{K}$的特征值与特征向量
对中心化核矩阵$\tilde{K}$进行特征值分解：  
$$\tilde{K} v = \lambda v$$  

其中，$\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_n \geq 0$为特征值，$v_1, v_2, ..., v_n$为对应的特征向量。


#### 步骤5：选择主成分并映射到低维空间
- 选择前$k$个最大的特征值对应的特征向量（主成分），记为$v_1, ..., v_k$；  
- 对每个特征向量$v_m$进行归一化：$\tilde{v}_m = \frac{v_m}{\sqrt{\lambda_m}}$（确保低维空间方差一致）；  
- 样本$x_i$在低维空间的投影为：  
  $$z_i = \left[ \sum_{j=1}^n \tilde{v}_{1j} k(x_i, x_j), \sum_{j=1}^n \tilde{v}_{2j} k(x_i, x_j), ..., \sum_{j=1}^n \tilde{v}_{kj} k(x_i, x_j) \right]^T$$  


### 四、Kernel PCA与普通PCA的对比
| 特性                | 普通PCA                          | Kernel PCA                      |
|---------------------|----------------------------------|---------------------------------|
| 处理数据类型        | 线性结构数据                    | 非线性结构数据                  |
| 映射方式            | 线性映射（特征向量投影）        | 非线性映射（核函数+高维PCA）    |
| 计算复杂度          | $O(n d^2 + d^3)$（$n$样本，$d$维度） | $O(n^3)$（依赖核矩阵规模）      |
| 可解释性            | 强（主成分对应原始特征的线性组合） | 弱（核函数映射难以直观解释）    |
| 适用场景            | 线性降维、去噪、特征提取        | 非线性降维（如瑞士卷、聚类结构） |


### 五、Kernel PCA的关键参数与调优
Kernel PCA的效果高度依赖核函数类型及参数，核心调优方向：  
1. **核函数选择**：  
   - 线性核：等价于普通PCA，适用于线性数据；  
   - RBF核：适用于大多数非线性场景，是默认选择；  
   - 多项式核：适用于已知多项式关系的数据（如二次分布）。  

2. **核参数调整**：  
   - RBF核的$\sigma$：控制局部性（$\sigma$越小，关注局部结构；越大，捕捉全局结构）；  
   - 多项式核的$d$（阶数）：$d$越大，非线性越强，但易过拟合。  

3. **主成分数量$k$**：通过特征值贡献率（$\sum_{i=1}^k \lambda_i / \sum_{i=1}^n \lambda_i$）选择，通常保留80%~90%的方差。


### 六、应用场景与局限性
#### 应用场景：
1. **非线性数据降维**：如“瑞士卷”数据集（将螺旋结构展开为2D）、图像特征（提取非线性边缘/纹理）；  
2. **分类/聚类预处理**：作为SVM、K-Means的前置步骤，降低维度并保留非线性特征；  
3. **去噪**：通过核函数过滤噪声，保留数据的非线性结构。  


#### 局限性：
1. **计算复杂度高**：核矩阵为$n \times n$，对大规模数据（$n \gg 10^4$）效率低；  
2. **可解释性差**：低维结果无法对应原始特征的线性组合，难以解释物理意义；  
3. **参数敏感**：核函数及参数选择对结果影响大，需大量实验调优。  


### 总结
Kernel PCA通过核技巧突破了普通PCA的线性限制，能有效处理非线性数据的降维问题。其核心是用核矩阵替代高维内积，通过特征分解实现非线性映射。尽管存在计算复杂度和可解释性的不足，但在非线性结构分析（如流形学习、复杂聚类）中仍具有不可替代的优势。理解核函数的作用和中心化步骤，是掌握Kernel PCA的关键。