一个[[RDD]]有多个分区
宽依赖和窄依赖是 Spark 中 RDD（弹性分布式数据集）之间的两种依赖关系，它们决定了计算过程中是否需要跨节点 Shuffle，进而直接影响作业性能与容错方式。

### 1. 窄依赖（Narrow Dependency）
- **定义**：父 RDD 的每个分区最多被子 RDD 的 **一个** 分区引用。  
- **特征**：无需 Shuffle，可在单个 Executor 内完成计算；节点失效时只需重算丢失的父分区，恢复成本低。  
- **常见形式**（三种）：

| 形式 | 说明 | 典型算子 | 示意图 |
|---|---|---|---|
| **一对一** | 子 RDD 分区与父 RDD 分区一一对应 | map、filter、mapPartitions | `p0 → c0`, `p1 → c1`, … |
| **范围依赖** | 子 RDD 分区连续地引用父 RDD 的一段分区 | union（两个 RDD 已按分区顺序合并） | `p0-2 → c0`, `p3-5 → c1` |
| **协同分区 Join** | **两个 RDD 使用相同的分区器且分区数相同**，可直接在本地对齐分区完成 Join，无需 Shuffle | `join`（前提是两个 RDD 均按同一 key 分区且分区数一致） | `c_i = join(p_i^A, p_i^B)` |

> ⚠️ 修正：原描述中的“协同分区 Join”并非只要分区数相同即可，而是要求 **分区器（Partitioner）完全相同** 且 **分区数相同**，否则仍会退化为宽依赖。

### 2. 宽依赖（Wide Dependency / Shuffle Dependency）
- **定义**：父 RDD 的 **一个** 分区被 **多个** 子 RDD 分区引用。  
- **特征**：必须执行 Shuffle（网络传输、磁盘 I/O、序列化/反序列化），是 Spark 作业的主要性能瓶颈；节点失效时往往需要重算整个父 RDD 的多个分区，恢复成本高。  
- **常见触发算子**：

| 算子 | 触发原因 |
|---|---|
| groupByKey / reduceByKey | 需要按 key 重分区 |
| distinct / intersection | 需要全局去重 |
| join（非协同分区） | 两个 RDD 的分区器不同或分区数不同 |

### 3. 性能与容错对比
| 维度 | 窄依赖 | 宽依赖 |
|---|---|---|
| **数据移动** | 无 Shuffle，本地计算 | 必须 Shuffle |
| **恢复代价** | 仅重算丢失的父分区 | 可能重算整个父 RDD 的多个分区 |
| **流水线** | 支持 | 不支持 |
| **内存占用** | 低 | 高（需缓存 Shuffle 中间结果） |

> 客观事实：宽依赖下的节点失效 **不一定** 需要“整体重新计算”，Spark 会根据血缘关系（lineage）重算 **丢失的下游分区及其必须的上游分区**，但范围通常大于窄依赖场景。

### 4. 小结
- **窄依赖** = 无 Shuffle、恢复细粒度、性能高。  
- **宽依赖** = 必须 Shuffle、恢复粗粒度、性能开销大。  
在开发 Spark 作业时，应 **尽量利用窄依赖**，例如提前对需要多次 Join 的 RDD 使用相同的分区器，或提前 `repartition`/`coalesce` 对齐分区，从而 **减少 Shuffle 次数**。