选取合适的多项式阶数对于回归的拟合程度会产生重要的影响。多项式阶数越高,越容易产生过拟合现象。

## 线性回归的基本假设
线性回归的基本假设:
(1)线性关系假设
(2)正态性假设, 指回归分析中的Y服从正态分布。
(3)独立性假设, 包含两个意思:
	①与某一个X值对应的一组Y值和与另一个X值对应的一组Y值之间没有关系, 彼此独立。②误差项独立, 不同的X所产生的误差之间应相互独立, 无自相关。	
(4)误差等分散性假设: 特定X水平的误差, 除了应呈随机化的常态分配,其变异量也应相等, 称为误差等分散性。


# OLS（Ordinary Least Squares，普通最小二乘法）估计量

OLS（Ordinary Least Squares，普通最小二乘法）估计量是线性回归模型中最常用的参数估计方法，其核心思想是通过最小化观测值与模型预测值之间的误差平方和来确定最优参数。下面从定义、数学表达、性质和应用四个方面进行详细解释：

## 一、OLS估计量的定义

在线性回归模型中，假设我们有 $n$ 个观测样本，每个样本包含 $p$ 个自变量 $X_1, X_2, ..., X_p$ 和一个因变量 $Y$，模型可以表示为：

$$
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + ... + \beta_p X_{ip} + \varepsilon_i \quad (i=1,2,...,n)
$$

其中：
- $Y_i$ 是第 $i$ 个观测的因变量值；
- $X_{ij}$ 是第 $i$ 个观测的第 $j$ 个自变量值；
- $\beta_0, \beta_1, ..., \beta_p$ 是待估计的参数（回归系数）；
- $\varepsilon_i$ 是随机误差项，通常假设其均值为 0，方差为 $\sigma^2$，且相互独立。

**OLS估计量**是指使以下**残差平方和**（Sum of Squared Residuals，SSR）最小的参数值 $\hat{\beta}_0, \hat{\beta}_1, ..., \hat{\beta}_p$：

$$
\text{SSR}(\beta) = \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^{n} \left(Y_i - (\beta_0 + \beta_1 X_{i1} + ... + \beta_p X_{ip})\right)^2
$$

其中 $\hat{Y}_i$ 是模型对 $Y_i$ 的预测值。

## 二、数学表达：矩阵形式与解析解

将模型用矩阵形式表示更为简洁：

$$
Y = X\beta + \varepsilon
$$

其中：
- $Y$ 是 $n \times 1$ 的因变量向量；
- $X$ 是 $n \times (p+1)$ 的设计矩阵（第一列全为 1，对应截距项 $\beta_0$）；
- $\beta$ 是 $(p+1) \times 1$ 的参数向量；
- $\varepsilon$ 是 $n \times 1$ 的误差向量。

OLS估计量 $\hat{\beta}$ 的解析解为：

$$
\hat{\beta} = (X^T X)^{-1} X^T Y
$$

推导过程：对 SSR 关于 $\beta$ 求偏导并令其为 0，得到正规方程（Normal Equations）：

$$
X^T X \beta = X^T Y
$$

当 $X^T X$ 可逆时，解得 $\hat{\beta} = (X^T X)^{-1} X^T Y$。

## 三、OLS估计量的性质

1. **线性性**  
   $\hat{\beta}$ 是 $Y$ 的线性组合，即 $\hat{\beta} = AY$，其中 $A = (X^T X)^{-1} X^T$ 是仅依赖于 $X$ 的矩阵。

2. **无偏性**  
   在经典假设下（误差项均值为 0，且与自变量不相关），OLS估计量是无偏的：

   $$
   E(\hat{\beta}) = \beta
   $$

   即估计量的期望等于真实参数值。

3. **最小方差性（BLUE性质）**  
   在所有线性无偏估计量中，OLS估计量具有最小方差，称为**最佳线性无偏估计量**（Best Linear Unbiased Estimator，BLUE）。这一性质由**高斯-马尔可夫定理**（Gauss-Markov Theorem）保证。

4. **一致性**  
   当样本量 $n$ 趋于无穷大时，OLS估计量 $\hat{\beta}$ 依概率收敛于真实参数 $\beta$。

5. **有效性**  
   在误差项服从正态分布的假设下，OLS估计量也是**最大似然估计量**（MLE），具有渐近有效性。

## 四、应用与局限性

1. **适用条件**  
   OLS估计量的优良性质依赖于以下假设（经典线性回归模型假设）：
   - **线性性**：因变量与自变量之间存在线性关系。
   - **严格外生性**：$E(\varepsilon_i | X) = 0$，即误差项的条件均值为 0。
   - **无完全多重共线性**：自变量之间不存在精确的线性关系，确保 $X^T X$ 可逆。
   - **同方差性**：$Var(\varepsilon_i | X) = \sigma^2$，即误差项的条件方差为常数。
   - **无自相关**：$Cov(\varepsilon_i, \varepsilon_j | X) = 0$，对所有 $i \neq j$。

2. **局限性**  
   - **对异常值敏感**：残差平方和对异常值的惩罚较大，可能导致估计结果偏移。
   - **多重共线性影响**：当自变量高度相关时，$(X^T X)^{-1}$ 的计算不稳定，估计量的方差增大。
   - **非线性关系处理能力有限**：若真实关系非线性，OLS可能无法捕捉数据的真实模式。

3. **改进方法**  
   针对上述问题，有以下改进方法：
   - **岭回归（Ridge Regression）**：通过添加正则化项缓解多重共线性。
   - **LASSO回归**：同时实现变量选择和参数估计。
   - **稳健回归（Robust Regression）**：使用对异常值不敏感的损失函数（如Huber损失）。

## 五、示例：简单线性回归的OLS估计

对于简单线性回归模型 $Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$，OLS估计量的计算公式为：

$$
\hat{\beta}_1 = \frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n} (X_i - \bar{X})^2}
$$

$$
\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}
$$

其中 $\bar{X}$ 和 $\bar{Y}$ 分别是 $X$ 和 $Y$ 的样本均值。

## 六、总结

OLS估计量是线性回归模型中最基础、最常用的参数估计方法，其核心是通过最小化残差平方和来求解最优参数。在满足经典假设的条件下，OLS估计量具有无偏性、最小方差性等优良性质。然而，实际应用中需注意模型假设的合理性，并根据数据特点选择适当的改进方法。