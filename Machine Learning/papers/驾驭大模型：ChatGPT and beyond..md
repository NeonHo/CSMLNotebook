# Abstract
- brief introduction of ChatGPT and Bert
- data's influence
- The use and non-use cases of LLM
	- acknowledge intensive
	- traditional NL understanding
	- NL generation
	- emergent abilities
- The importance of Data
- The specific challenges
- 
- essential considerations for deployment of LLMs:
	- the impact of spurious biases
	- efficiency
	- cost
	- latency
# Introduction
- rapid development recently
- address multi NLP tasks.
- Effectively and efficiently requires understanding.

- Working with LLMs in downstream NLP tasks.
- Why or why not use LLMs
- How to select the most suitable LLM.
	- Model size
	- computational requirements
	- existing domain-specific pre-trained models.
- For better leverage the power

- Paper Structure:
	- brief introduction to LLMs
		- GPT
		- BERT
	- critical factors from the data
		- pre-training data
		- training/tuning data
		- test data
	- various concrete NLP tasks
		- knowledge-intensive task
			- Q&A
			- conversation
			- inference
		- traditional NLU(Nature Language Understanding) tasks
			- Named Entity Recognition (NER)[[Named Entity Recognition]]
		- generation task
- Analyze the abilities of LLM.
	- LLMs v.s.  fine-tuned models.
	- LLMs
		- huge language  models
		- pre-trained on large amounts of datasets 
		- without tuning on data for specific tasks
	- fine-tuned models  less than 20B params
		- smaller
		- pretrained
		- further tuned on a smaller, task-specific dataset.
- summarize
	- Natural language understanding.
		- generalize to out-of-distribution data
		- with few training data.
	- Natural language generation
		- create text for various applications
	- Knowledge-intensive tasks
		- requiring domain-specific expertise
		- requiring general world knowledge.
	- Reasoning ability
		- decision-making
		- problem-solving
# Practical Guide for Models
- sota LLMs
	- differ in 
		- training strategies
		- model architectures
		- use cases
	- 2 types
		- encoder-decoder and encoder-only
			- [[Masked Language Models]]
			- Discriminative [[Generative Model & Discriminative Model]]
			- Predict masked words
		- decoder-only
			- [[Autoregressive Language Models]]
			- 
		-
	- evolutionary tree
		- decoder-only (gradually dominating)
			- 2021 game-changing LLMs GPT-3
			- boom
		- encoder-only
			- BERT
			- fade away
		- OpenAI leadership
		- Meta
			- open-source LLMs
		- close-source is the tendency
			- PaLM
			- LaMDA
			- GPT-4
			- API-based research become the predominant method in the academic community
		- encoder-decoder remain promising.
			- Most are open-sourced
			- Google contributions
			- flexibility and versatility of decoder-only make it less promising.
		