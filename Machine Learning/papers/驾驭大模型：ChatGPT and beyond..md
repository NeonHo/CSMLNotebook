# Abstract
- brief introduction of ChatGPT and Bert
- data's influence
- The use and non-use cases of LLM
	- acknowledge intensive
	- traditional NL understanding
	- NL generation
	- emergent abilities
- The importance of Data
- The specific challenges
- 
- essential considerations for deployment of LLMs:
	- the impact of spurious biases
	- efficiency
	- cost
	- latency
# Introduction
- rapid development recently
- address multi NLP tasks.
- Effectively and efficiently requires understanding.

- Working with LLMs in downstream NLP tasks.
- Why or why not use LLMs
- How to select the most suitable LLM.
	- Model size
	- computational requirements
	- existing domain-specific pre-trained models.
- For better leverage the power

- Paper Structure:
	- brief introduction to LLMs
		- GPT
		- BERT
	- critical factors from the data
		- pre-training data
		- training/tuning data
		- test data
	- various concrete NLP tasks
		- knowledge-intensive task
			- Q&A
			- conversation
			- inference
		- traditional NLU(Nature Language Understanding) tasks
			- Named Entity Recognition (NER)[[Named Entity Recognition]]
		- generation task
- Analyze the abilities of LLM.
	- LLMs v.s.  fine-tuned  models.
	- LLMs 
		- huge language models
		- pretrained on large amounts of datasets 
		- without tuning on data for specific tasks
	- fine-tuned models
		- smaller
		- pretrained
		- further tuned on a smaller, task-specific dataset.
- summarize
	- Natural language understanding.
		- generalize to out-of-distribution data
		- 
	- Natural language generation
	- 
