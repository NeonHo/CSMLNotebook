# 1. Why we need ensemble models?
Maybe, the single model in the Machine Learning can not performance idealy.
For example, only one model may not predict the result accurately or may only performance accurately in an small subset.
We can use some tricks to lift the performance of the whole model by compose several ‘silly’ models.
# 2. What are the ensemble models?
There are 3 existing methods to improve the performance.
The one is called ‘bagging’, the second is called ‘boosting’, and the other is called ‘stacking’.
**Bagging** is parrallel-connection among homogeneous  weak models. At last we combine the predictive results by averaging all the results. The only advantage of bagging is to reduce the variance to make it lower than any components.
**Boosting** is series-connection among homogeneous weak models. The final result can be predicted by the some specific strategies. For instance, the current model is training with the error caused by the previous one.
**Stacking** is training various models parallelly, and the final result is generated by a meta-model after all weak models.
Not only these 2 ensemble tricks can reduce the variance of  whole models to avoid **overfitting**, but also can reduce the bias to avoid **under-fitting**.
## 2.1. Bagging
As the description above, bagging is a parallel method which trains several **not bad but not robust** model independently from each other at the mean while, so that we can get a more robust model.
So, let’ s be more specific!
How to train this kind of ensemble models? This question can be divided into 3 sub-questions.
1. How to build training set for the ensemble models?
2. How to ensemble the modes for robust?
3. Is there any typical architecture we need to know?
### 2.1.1. **Q1: **How to build training set for the ensemble models?
For the first question, we want actually know how the training sets should be like? 
What dataset the sub-models (or weak learners) need?
First of all, it is easy to imagine that we need training sets with different samples.
If not so, all sub-models have the same structure and the same training set, even the same training schedule, which is meaningless.
So for a higher diversity, we need a series of training sets which are different from the others.
A series of training sets with a higher diversity can help each learners to capture features and patterns from different views.
This can help the ensemble model which consists of the trained sub-models become **more robust** and with **lower variance**, so that it can be **prevented from over-fitting**.

However, if the training sets are highly different from the true distribution in the real word, the observations in these sets will break the basic assumption:
 *The dataset we captured is Independent and Identically Distributed ($i.i.d.$) with the real world data.* 
So, the second request is that: 
“Given a initial dataset which is i.i.d with real world, each training set is sampled from the initial dataset.”

Before this request, we need to guarantee that our initial dataset is large enough to have the ability to represent the real world distribution. 
Before the demands of diversity, we need to guarantee that the initial dataset is much larger than the sub-sets we sampled, so that the sub-sets are easily independent from the other.
Based on these demands above, we need several **independent** and **representative** training sets for each models.
But in almost all real tasks, we can not usually capture enough observations to present the real part of the world we are interested in.
So we need a skill to construct our training sets which meet our demands above for all of the sub-models.
The skill is called **Bootstrapping**.
### 2.1.2. **A1: ** Bootstrapping
If we assume that we want train $L$ sub-models. 
It is easy to understand what the Bootstrapping is doing.
Just *sampling $B$ [[Observations]]  with replacement from initial dataset with $N$ samples in it.*
The $B$ observations are used to compose a **Bootstrap Sample**. 
This sampling need to be done for $L$imes to create $L$ bootstrap samples to feed each sub-model.
$$
\{z_1^{(1)}, ..., z_B^{(1)}\},...,
\{z_1^{(l)}, ..., z_B^{(l)}\},...,
\{z_1^{(L)}, ..., z_B^{(L)}\}
$$
$z_i^{(l)}$ means the $i{th}$ observation in the $l$ bootstrap sample.
According to the sampling way, we can easily conclude that these sub-sets are approximately Independent and Identically Distribution ($i.i.d.$).
### 2.1.3. **Q2: **How to ensemble the modes for robust?
I guess that you must have imagine that if your model is too  limited, it is natural that put several “nerd” (not robust) models and output the average of their predictions.
### 2.1.4. **A2: ** Bagging your limited models!
This may be a intuition that averaging the outputs of your models is better, but why?
#### 2.1.4.1. Why average?
Acturally, average can reduce variance overall but preserve expected value if our training sets are $i.i.d$.
A easy proof is that: 
If random variables, $X$ and $Y$, are $i.i.d$, $E(X)$ and $E(Y)$ denotes expected values, $Var(X)$ and $Var(Y)$ denotes variance.
$Z$ is the average of $X$ and $Y$, then:
$$
\begin{array}{}
E(Z)=\frac{E(X)+E(Y)}{2}\\
Var(Z)=\frac{Var(X)}{4}+\frac{Var(Y)}{4}
\end{array}
$$
We all know that if $X$ and $Y$ is $i.i.d$, then $E(X)=E(Y)$, $Var(X)=Var(Y)$.
So $E(Z)=E(X)=E(Y)$ and $Var(Z)=\frac{Var(X)}{2}=\frac{Var(Y)}{2}$.
#### 2.1.4.2. How bagging?
The average above is **bagging**!
The only difficulty left is the good training sets for the models to fit for a lower variance overall.
We need $i.i.d$ training sets which are approximate to real world, but we don’ t have such a ideal initial dataset for sampling.
Luckily, bootstrap solves this problem as much as possible.
If we use bootstrapping, we can create bootstrap samples which are almost $i.i.d$ with real world.
We will fit the $lth$ weak learner $w_l(\cdot)$ on one bootstrap sample 
$\{z_1^{(l)}, ..., z_B^{(l)}\}$.
After we had trained all weak learners, $w_1(\cdot),...,w_l(\cdot),...w_L(\cdot)$, we can do “specific average” on the outputs from these learners in some way as our final predictive result in the end.
So what is the “specific average”?
#### 2.1.4.3. How to average the outputs?
##### 2.1.4.3.1. **For Regression: Literally Averaging.**
$$
\frac{1}{L}\sum_{l=1}^{L}w_l(\cdot)
$$
##### 2.1.4.3.2. **For Classification: Hard Voting & Soft Voting.**
As for hard voting, we just regard the majority of the votes from all learners as the final prediction.
$$
v = \arg \max_c \sum_{l=1}^L \delta(c_l, c)
$$
where,
 
|simble|meaning|
|:---:|:---:|
| $v$ | the final vote |
| $c$ | the class which is corresponding to majority votes. |
| $c_1, ..., c_l, ..., c_L$ | the class outputs from learners |
| $\delta$ | the Dirac Delta function. |

Reference: [[Dirac Delta Function]] 
As for soft voting, we can calculate average as follows:
$$
v = \arg \max_c \sum_{l=1}^L p(c_l, c)
$$
where:
$$
p\left( c_{i},c\right) =\begin{cases}p_{i},ifc_{i}=c\\
0,other\end{cases}
$$
#### 2.1.4.4. Summary for bagging.
The final result is more robust than the outputs of the learners.
If you want your bagging is more flexible, you can **attach weights** to the outputs of your weak learners!
To summarize, bagging is **a series weak learners were parallel connected, with an “average” aggregation at the tail.**, this is the ensemble model.
### 2.1.5. **Q3:** Is there any typical architecture we need to know?
If bagging is a good idea, what typical or famous method is based on this idea?
### 2.1.6. **A3:** Random Forest

Forest: a strong learner which is composed of several trees.
