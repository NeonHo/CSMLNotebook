# 1. 为什么需要知道几种经典的优化问题？
方便我们面对新的优化问题时有求解思路。
# 2. 什么是无约束优化问题？
$$
\min_{\theta}L(\theta)
$$
其中目标函数是光滑的. 那么这就是一个无约束优化问题。
# 3. 如何求解无约束优化问题？
根据不同的适用场景，去选择经典的优化算法求解。
## 3.1. 直接法
条件：目标函数是凸函数[[第2节 机器学习中的优化问题#2.1.1. 凸函数]]，目标函数求导为零有闭式解。
### 3.1.1. 目标函数为凸函数
梯度为零处就是最优解。
### 3.1.2. 求导为零有闭式解
[[拓展3 闭式解]]
## 3.2. 迭代法
假设我们当前找到的最优参数组合估计值为$\theta_{t}$，更好的参数组合估计值为$\theta_t + \delta$ 。
希望求解的问题是:
$$
\delta_t = \arg \min_{\delta} L(\theta_t+\delta)
$$
通过迭代反复的修正我们的估计.
### 3.2.1. 一阶法
一阶法实际上就是对函数做一阶泰勒展开，但是一阶泰勒展开只有在$\delta$比较小时才能比较准地近似，所以一般还要额外加一个正则项。
原因如下：

通常情况下，我们会对模型的损失函数进行泰勒展开，然后根据展开后的一阶导数来决定步长大小。

那么，为什么在$L(\theta_t + \delta) \approx L(\theta_t)+\nabla L(\theta_t)^T\delta$中，$\delta$比较小时近似才比较准呢？这是因为泰勒展开是在当前点$\theta_t$处进行的，展开式只在该点附近有效。当$\delta$较小时，展开式在$\theta_t+\delta$处的误差较小，因此近似较为准确。而当$\delta$较大时，展开式在$\theta_t+\delta$处的误差会逐渐积累，近似就会变得不准确。

因此，当步长过大时，梯度下降算法可能会无法收敛，或者收敛速度会很慢。为了保证算法能够稳定地收敛，我们通常需要选择合适的步长，以及合适的迭代次数。

一阶法的迭代公式：
$$
\begin{array}{lr}
\delta_t=-\alpha\nabla L(\theta_t)\\
\theta_{t+1}=\theta_{t} -\alpha\nabla L(\theta_t)
\end{array}
$$

### 3.2.2. 二阶法

#### 3.2.2.1. 使用二阶法的原因
在机器学习中，我们通常使用梯度下降算法来更新模型的参数。梯度下降算法的核心思想是在每一次迭代中，沿着损失函数的梯度方向进行参数更新，以最小化损失函数。一阶导数的梯度下降算法只利用了梯度的信息，而二阶导数的梯度下降算法，除了利用梯度信息，还利用了二阶导数的信息。因此，二阶导数的梯度下降算法通常比一阶导数的梯度下降算法收敛得更快。

具体地说，二阶导数的梯度下降算法利用了Hessian矩阵来更新参数。Hessian矩阵是损失函数的二阶导数矩阵，它包含了更多的信息，可以更准确地描述损失函数的形状。在每一次迭代中，二阶导数的梯度下降算法会计算Hessian矩阵和梯度的乘积，得到一个更新方向，然后沿着这个方向更新参数。相比之下，一阶导数的梯度下降算法只利用了梯度的信息，它可能会在某些情况下收敛得很慢。

二阶导数的梯度下降算法的一个缺点是，计算Hessian矩阵需要更多的计算量和存储空间。此外，在高维空间中，Hessian矩阵通常是非常稀疏的，因此需要使用一些稀疏矩阵技巧来计算和存储Hessian矩阵。

总的来说，二阶导数的梯度下降算法可以更快地收敛，但是需要更多的计算量和存储空间。在实际应用中，我们需要根据具体的问题选择合适的优化算法。
$$
\begin{array}{lr}
\delta_t=-\nabla^2L(\theta_t)^{-1}\nabla L(\theta_t)\\
\theta_{t+1}=\theta_{t}-\nabla^2L(\theta_t)^{-1}\nabla L(\theta_t)\\
\end{array}
$$

#### 3.2.2.2. 牛顿法
泰勒展开式做二阶展开。也称为牛顿法。
这种方法的收敛速度一般要快于一阶法，但是海森矩阵的计算复杂度大，而且当函数非凸时，二阶法很有可能会收敛到鞍点。

#### 3.2.2.3. 收敛到鞍点

收敛到鞍点的原因：

在二阶导数下降中，我们使用二阶导数矩阵（Hessian矩阵）来更新参数。当损失函数是凸函数时，二阶导数矩阵是正定的，因此二阶导数下降可以保证收敛到全局最小值。

但是，当损失函数是非凸函数时，Hessian矩阵可能不是正定的，而是包含正负特征值的矩阵。在这种情况下，二阶导数下降可能会收敛到局部最小值、鞍点或者无法收敛。特别是在高维空间中，鞍点比局部最小值更为普遍。

鞍点是指函数在某一点的梯度为0，但该点不是全局最小值或最大值的点。在鞍点处，函数表现为一个局部最小值和一个局部最大值的组合。因此，当我们使用二阶导数下降算法时，如果初始点位于鞍点附近，就可能会收敛到鞍点而不是全局最小值。

为了避免在非凸情况下陷入鞍点，我们可以使用其他的优化算法，例如随机梯度下降、Adam等，这些算法通常具有更好的鲁棒性和收敛速度。此外，我们还可以使用一些启发式方法来初始化参数，例如Xavier初始化、He初始化等，以减少陷入鞍点的概率。
